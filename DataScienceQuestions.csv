Question,Response,Source,Type,Track
"Tell me about a recent data analysis project you worked on?  
What were the interesting or unexpected findings that you had from the project?
How did you end up productionizing it if at all or what was the end result?",,,Experience,General
"Suppose we offered 3 promotions at the Google Play Store.
How would we know if the promotions were successful or not?",,,Case Study,Quantitative
Describe different approaches to ranking photos at a given location and evaluating these ranking algorithms.,,,Case Study,Quantitative
"Suppose you want to calculate the probability that game 7 is reached.  How would you use Monte Carlo simulation to estimate this probability and using transition matrices?  You have two teams A/B with the following probabilities:

Pr(A wins at time t=1) = p1
Pr(A wins at time t=k | A wins at time t=k-1) = p11
Pr(A wins at time t=k | B wins at time t=k-1) = p12
Pr(B wins at time t=k | A wins at time t=k-1) = p21
Pr(B wins at time t=k | B wins at time t=k-1) = p22
",,,Programming,Quantitative
"People are submitting contradictory edits for submissions to different places for a maps app where some say it is open and others say it is closed.
How would you assess the reliability of the submissions?",,,Case Study,Quantitative
What are different types of models used for analyzing and evaluating the causal impacts of different product or marketing strategies?,,,Knowledge,Quantitative
What is the functional form for a maximum likelihood estimation of a logit regression model?,,,Knowledge,Quantitative
What are the assumptions required for linear regression?,"There are four major assumptions: 1. There is a linear relationship between the dependent variables and the regressors, meaning the model you are creating actually fits the data, 2. The errors or residuals of the data are normally distributed and independent from each other, 3. There is minimal multicollinearity between explanatory variables, and 4. Homoscedasticity. This means the variance around the regression line is the same for all values of the predictor variable.",springboard,,
What is the Central Limit Theorem and why is it important?,"Suppose that we are interested in estimating the average height among all people. Collecting data for every person in the world is impossible. While we can’t obtain a height measurement from everyone in the population, we can still sample some people. The question now becomes, what can we say about the average height of the entire population given a single sample. The Central Limit Theorem addresses this question exactly.",springboard,,
What is sampling? How many sampling methods do you know?,"Data sampling is a statistical analysis technique used to select, manipulate and analyze a representative subset of data points to identify patterns and trends in the larger data set being examined.”",springboard,,
What is the difference between type I vs type II error?,"A type I error occurs when the null hypothesis is true, but is rejected. A type II error occurs when the null hypothesis is false, but erroneously fails to be rejected.",springboard,,
"What is linear regression? What do the terms p-value, coefficient, and r-squared value mean? What is the significance of each of these components?","A linear regression is a good tool for quick predictive analysis: for example, the price of a house depends on a myriad of factors, such as its size or its location. In order to see the relationship between these variables, we need to build a linear regression, which predicts the line of best fit between them and can help conclude whether or not these two factors have a positive or negative relationship",springboard,,
What are the assumptions required for linear regression?,"There are four major assumptions: 1. There is a linear relationship between the dependent variables and the regressors, meaning the model you are creating actually fits the data, 2. The errors or residuals of the data are normally distributed and independent from each other, 3. There is minimal multicollinearity between explanatory variables, and 4. Homoscedasticity. This means the variance around the regression line is the same for all values of the predictor variable.",springboard,,
What is a statistical interaction?,"”Basically, an interaction is when the effect of one factor (input variable) on the dependent variable (output variable) differs among levels of another factor.”",springboard,,
What is selection bias?,"“Selection (or ‘sampling’) bias occurs in an ‘active,’ sense when the sample data that is gathered and prepared for modeling has characteristics that are not representative of the true, future population of cases the model will see. That is, active selection bias occurs when a subset of the data are systematically (i.e., non-randomly) excluded from analysis.”",springboard,,
What is an example of a data set with a non-Gaussian distribution?,"“The Gaussian distribution is part of the Exponential family of distributions, but there are a lot more of them, with the same sort of ease of use, in many cases, and if the person doing the machine learning has a solid grounding in statistics, they can be utilized where appropriate.” ",springboard,,
What is the Binomial Probability Formula?,“The binomial distribution consists of the probabilities of each of the possible numbers of successes on N trials for independent events that each have a probability of π (the Greek letter pi) of occurring.”,springboard,,
With which programming languages and environments are you most comfortable working?,,springboard,,
What are some pros and cons about your favorite statistical software?,,springboard,,
Tell me about an original algorithm you’ve created.,,springboard,,
Describe a data science project in which you worked with a substantial programming component. What did you learn from that experience?,,springboard,,
Do you contribute to any open-source projects?,,springboard,,
How would you clean a data set in (insert language here)?,,springboard,,
Tell me about the coding you did during your last project?,,springboard,,
What are two main components of the Hadoop framework?,"The Hadoop Distributed File System (HDFS), MapReduce, and YARN.",springboard,,
Explain how MapReduce works as simply as possible.,MapReduce is a programming model that enables distributed processing of large data sets on compute clusters of commodity hardware. Hadoop MapReduce first performs mapping which involves splitting a large file into pieces to make another set of data.,springboard,,
How would you sort a large list of numbers?,,springboard,,
Say you’re given a large data set. What would be your plan for dealing with outliers? How about missing values? How about transformations?,,springboard,,
What modules/libraries are you most familiar with? What do you like or dislike about them?,,springboard,,
"In Python, how is memory managed?",,springboard,,
"In Python, memory is managed in a private heap space. This means that all the objects and data structures will be located in a private heap. However, the programmer won’t be allowed to access this heap. Instead, the Python interpreter will handle it. At the same time, the core API will enable access to some Python tools for the programmer to start coding. The memory manager will allocate the heap space for the Python objects while the inbuilt garbage collector will recycle all the memory that’s not being used to boost available heap space. ",,springboard,,
What are the supported data types in Python?,,springboard,,
"“Python’s built-in (or standard) data types can be grouped into several classes. Sticking to the hierarchy scheme used in the official Python documentation these are numeric types, sequences, sets and mappings.”",,springboard,,
What is the difference between a tuple and a list in Python?,“Apart from tuples being immutable there is also a semantic distinction that should guide their usage.”,springboard,,
What are the different types of sorting algorithms available in R language?,"There are insertion, bubble, and selection sorting algorithms.",springboard,,
What are the different data objects in R?,"“R objects can store values as different core data types (referred to as modes in R jargon); these include numeric (both integer and double), character and logical.” ",springboard,,
What packages are you most familiar with? What do you like or dislike about them?,,springboard,,
How do you access the element in the 2nd column and 4th row of a matrix named M?,"We can access elements of a matrix using the square bracket [ indexing method. Elements can be accessed as var[row, column]",springboard,,
What is the command used to store R objects in a file?,,springboard,,
What is the best way to use Hadoop and R together for analysis?,“Hadoop and R complement each other quite well in terms of visualization and analytics of big data. There are four different ways of using Hadoop and R together.”,springboard,,
How do you split a continuous variable into different groups/ranks in R?,,springboard,,
Write a function in R language to replace the missing value in a vector with the mean of that vector.,,springboard,,
What is the purpose of the group functions in SQL? Give some examples of group functions.,"Group functions are necessary to get summary statistics of a data set. COUNT, MAX, MIN, AVG, SUM, and DISTINCT are all group functions.",springboard,,
"Tell me the difference between an inner join, left join/right join, and union.","“In a Venn diagram the inner join is when both tables have a match, a left join is when there is a match in the left table and the right table is null, a right join is the opposite of a left join, and a full join is all of the data combined.”",springboard,,
What does UNION do? What is the difference between UNION and UNION ALL?,"“UNION removes duplicate records (where all columns in the results are the same), UNION ALL does not.” ",springboard,,
What is the difference between SQL and MySQL or SQL Server?,"SQL stands for Structured Query Language. It’s a standard language for accessing and manipulating databases. MySQL is a database management system, like SQL Server, Oracle, Informix, Postgres, etc.",springboard,,
"If a table contains duplicate rows, does a query result display the duplicate values by default? How can you eliminate duplicate rows from a query result?",Yes. One way you can eliminate duplicate rows with the DISTINCT clause.,springboard,,
Tell me about how you designed a model for a past employer or client.,,springboard,,
What are your favorite data visualization techniques?,,springboard,,
How would you effectively represent data with 5 dimensions?,,springboard,,
How is k-NN different from k-means clustering?,"k-NN, or k-nearest neighbors is a classification algorithm, where the k is an integer describing the number of neighboring data points that influence the classification of a given observation. K-means is a clustering algorithm, where the k is an integer describing the number of clusters to be created from the given data.",springboard,,
How would you create a logistic regression model?,,springboard,,
Have you used a time series model? Do you understand cross-correlations with time lags?,,springboard,,
"Explain the 80/20 rule, and tell me about its importance in model validation.",“People usually tend to start with a 80-20% split (80% training set – 20% test set) and split the training set once more into a 80-20% ratio to create the validation set.”,springboard,,
Explain what precision and recall are. How do they relate to the ROC curve?,"Recall describes what percentage of true positives are described as positive by the model. Precision describes what percent of positive predictions were correct. The ROC curve shows the relationship between model recall and specificity–specificity being a measure of the percent of true negatives being described as negative by the model. Recall, precision, and the ROC are measures used to identify how useful a given classification model is. Read more here.",springboard,,
Explain the difference between L1 and L2 regularization methods.,“A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term.” ,springboard,,
What is root cause analysis?,"All of us dread that meeting where the boss asks ‘why is revenue down?’ The only thing worse than that question is not having any answers! There are many changes happening in your business every day, and often you will want to understand exactly what is driving a given change — especially if it is unexpected. Understanding the underlying causes of change is known as root cause analysis.",springboard,,
What are hash table collisions?,"If the range of key values is larger than the size of our hash table, which is usually always the case, then we must account for the possibility that two different records with two different keys can hash to the same table index. There are a few different ways to resolve this issue. In hash table vernacular, this solution implemented is referred to as collision resolution.",springboard,,
What is an exact test?,"In statistics, an exact (significance) test is a test where all assumptions, upon which the derivation of the distribution of the test statistic is based, are met as opposed to an approximate test (in which the approximation may be made as close as desired by making the sample size big enough). This will result in a significance test that will have a false rejection rate always equal to the significance level of the test. For example an exact test at significance level 5% will in the long run reject true null hypotheses exactly 5% of the time.",springboard,,
"In your opinion, which is more important when designing a machine learning model: model performance or model accuracy?","What is one way that you would handle an imbalanced data set that’s being used for prediction (i.e., vastly more negative classes than positive classes)?",springboard,,
How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression?,,springboard,,
I have two models of comparable accuracy and computational performance. Which one should I choose for production and why?,,springboard,,
How do you deal with sparsity?,,springboard,,
Is it better to spend five days developing a 90-percent accurate solution or 10 days for 100-percent accuracy?,,springboard,,
What are some situations where a general linear model fails?,,springboard,,
Do you think 50 small decision trees are better than a large one? Why?,,springboard,,
"When modifying an algorithm, how do you know that your changes are an improvement over not doing anything?",,springboard,,
Is it better to have too many false positives or too many false negatives?,,springboard,,
Tell me about a time when you took initiative.,,springboard,,
Tell me about a time when you had to overcome a dilemma.,,springboard,,
Tell me about a time when you resolved a conflict.,,springboard,,
Tell me about a time you failed and what you have learned from it.,,springboard,,
Tell me about (a job on your resume). Why did you choose to do it and what do you like most about it?,,springboard,,
Tell me about a challenge you have overcome while working on a group project.,,springboard,,
"When you encountered a tedious, boring task, how would you deal with it and motivate yourself to complete it?",,springboard,,
What have you done in the past to make a client satisfied/happy?,,springboard,,
What have you done in your previous job that you are really proud of?,,springboard,,
What do you do when your personal life is running over into your work life?,,springboard,,
Which data scientists do you admire most? Which startups?,There are plenty of amazing data scientists to choose from—take a look at this article on top data science influencers for interesting information about some of the top data scientists in the world.,springboard,,
What do you think makes a good data scientist?,,springboard,,
How did you become interested in data science?,,springboard,,
Give a few examples of “best practices” in data science.,,springboard,,
What is the latest data science book / article you read? What is the latest data mining conference / webinar / class / workshop / training you attended?,,springboard,,
What’s a project you would want to work on at our company?,,springboard,,
What unique skills do you think you’d bring to the team?,,springboard,,
What data would you love to acquire if there were no limitations?,,springboard,,
Have you ever thought about creating your own startup? Around which idea / concept?,,springboard,,
What can your hobbies tell me that your resume can’t?,,springboard,,
What are your top 5 predictions for the next 20 years?,,springboard,,
What did you do today? Or what did you do this week / last week?,,springboard,,
How would you come up with a solution to identify plagiarism?,,springboard,,
How many “useful” votes will a Yelp review receive?,,springboard,,
How do you detect individual paid accounts shared by multiple users?,,springboard,,
You are about to send a million emails. How do you optimize delivery? How do you optimize response?,,springboard,,
"You have a data set containing 100,000 rows and 100 columns, with one of those columns being our dependent variable for a problem we’d like to solve. How can we quickly identify which columns will be helpful in predicting the dependent variable. Identify two techniques and explain them to me as though I were 5 years old.",,springboard,,
"How would you detect bogus reviews, or bogus Facebook accounts used for bad purposes?","This is an opportunity to showcase your knowledge of machine learning algorithms; specifically, sentiment analysis and text analysis algorithms. Showcase your knowledge of fraudulent behavior—what are the abnormal behaviors that can typically be seen from fraudulent accounts?",springboard,,
"How would you perform clustering on a million unique keywords, assuming you have 10 million data points—each one consisting of two keywords, and a metric measuring how similar these two keywords are? How would you create this 10 million data points table in the first place?",,springboard,,
"How would you optimize a web crawler to run much faster, extract better information, and better summarize data to produce cleaner databases?",,springboard,,
"You are compiling a report for user content uploaded every month and notice a spike in uploads in October. In particular, a spike in picture uploads. What might you think is the cause of this, and how would you test it?",Hypothesis: the photos are Halloween pictures. Test: look at upload trends in countries that do not observe Halloween as a sort of counter-factual analysis.  We cannot say what has caused the spike since causal relationship cannot be established with observed data. But we can compare the averages of all the months by performing a hypothesis testing and rejecting the null hypothesis if the F1 score is significant.,glassdoor,,
"How do you take millions of users with 100's of transactions each, amongst 10k's of products and group the users together in a meaningful segments?",,glassdoor,,
How would you build and test a metric to compare two user's ranked lists of movie/tv show preferences?,,glassdoor,,
"How would you test if survey responses were filled at random by certain individuals, as opposed to truthful selections?",,udacity,,
What do you think a data scientist is/does?,,udacity,,
What do you think are the most important skills for a data scientist to have?,,udacity,,
"Which machine learning model (classification vs. regression, for example) to use given a particular problem.",,udacity,,
"How to go about training, testing, and validating results. Different ways of controlling for model complexity.",,udacity,,
"How to model a quantity that you can’t directly observe (using Bayesian approaches, for example, and when doing so, how to choose prior distributions).",,udacity,,
"What are various numerical optimization techniques (maximum likelihood, maximum a posteriori).",,udacity,,
"What types of data are important for a particular set of business needs, how you would go about collecting that data.",,udacity,,
"Dealing with correlated features in your data set, how to reduce the dimensionality of data.",,udacity,,
Linear/polynomial regression,,udacity,,
Decision trees,,udacity,,
Dimensionality reduction,,udacity,,
Clustering,,udacity,,
What’s a project you would want to work on at our company?,,udacity,,
What data would you go after to start working on it?,,udacity,,
What unique skills do you think you’d bring to the team?,,udacity,,
how do you detect outliers?,,dezyre,,
"What is the biggest data set that you processed, and how did you process it, what were the results?",,dezyre,,
Tell me two success stories about your analytic or computer science projects? How was lift (or success) measured?,,dezyre,,
"What is: lift, KPI, robustness, model fitting, design of experiments, 80/20 rule?",,dezyre,,
"What is: collaborative filtering, n-grams, map reduce, cosine distance?",,dezyre,,
"How to optimize a web crawler to run much faster, extract better information, and better summarize data to produce cleaner databases?",,dezyre,,
How would you come up with a solution to identify plagiarism?,,dezyre,,
How to detect individual paid accounts shared by multiple users?,,dezyre,,
Should click data be handled in real time? Why? In which contexts?,,dezyre,,
"What is better: good data or good models? And how do you define ""good""? Is there a universal good model? Are there any models that are definitely not so good?",,dezyre,,
What is probabilistic merging (AKA fuzzy merging)? Is it easier to handle with SQL or other languages? Which languages would you choose for semi-structured text data reconciliation? ,,dezyre,,
How do you handle missing data? What imputation techniques do you recommend?,,dezyre,,
What is your favorite programming language / vendor? why?,,dezyre,,
Tell me 3 things positive and 3 things negative about your favorite statistical software.,,dezyre,,
"Compare SAS, R, Python, Perl",,dezyre,,
What is the curse of big data?,,dezyre,,
Have you been involved in database design and data modeling?,,dezyre,,
Have you been involved in dashboard creation and metric selection? What do you think about Birt?,,dezyre,,
What features of Teradata do you like?,,dezyre,,
You are about to send one million email (marketing campaign). How do you optimze delivery? How do you optimize response? Can you optimize both separately? (answer: not really),,dezyre,,
"Toad or Brio or any other similar clients are quite inefficient to query Oracle databases. Why? How would you do to increase speed by a factor 10, and be able to handle far bigger outputs? ",,dezyre,,
How would you turn unstructured data into structured data? Is it really necessary? Is it OK to store data as flat text files rather than in an SQL-powered RDBMS?,,dezyre,,
What are hash table collisions? How is it avoided? How frequently does it happen?,,dezyre,,
How to make sure a mapreduce application has good load balance? What is load balance?,,dezyre,,
Examples where mapreduce does not work? Examples where it works very well? What are the security issues involved with the cloud? What do you think of EMC's solution offering an hybrid approach - both internal and external cloud - to mitigate the risks and offer other advantages (which ones)?,,dezyre,,
"Is it better to have 100 small hash tables or one big hash table, in memory, in terms of access speed (assuming both fit within RAM)? What do you think about in-database analytics?",,dezyre,,
Why is naive Bayes so bad? How would you improve a spam detection algorithm that uses naive Bayes?,,dezyre,,
Have you been working with white lists? Positive rules? (In the context of fraud or spam detection),,dezyre,,
What is star schema? Lookup tables? ,,dezyre,,
"Can you perform logistic regression with Excel? (yes) How? (use linest on log-transformed data)? Would the result be good? (Excel has numerical issues, but it's very interactive)",,dezyre,,
"Have you optimized code or algorithms for speed: in SQL, Perl, C++, Python etc. How, and by how much?",,dezyre,,
"Is it better to spend 5 days developing a 90% accurate solution, or 10 days for 100% accuracy? Depends on the context?",,dezyre,,
"Define: quality assurance, six sigma, design of experiments. Give examples of good and bad designs of experiments.",,dezyre,,
"What are the drawbacks of general linear model? Are you familiar with alternatives (Lasso, ridge regression, boosted trees)?",,dezyre,,
Do you think 50 small decision trees are better than a large one? Why?,,dezyre,,
"Is actuarial science not a branch of statistics (survival analysis)? If not, how so?",,dezyre,,
"Give examples of data that does not have a Gaussian distribution, nor log-normal. Give examples of data that has a very chaotic distribution?",,dezyre,,
Why is mean square error a bad measure of model performance? What would you suggest instead?,,dezyre,,
How can you prove that one improvement you've brought to an algorithm is really an improvement over not doing anything? Are you familiar with A/B testing?,,dezyre,,
"What is sensitivity analysis? Is it better to have low sensitivity (that is, great robustness) and low predictive power, or the other way around? How to perform good cross-validation? What do you think about the idea of injecting noise in your data set to test the sensitivity of your models?",,dezyre,,
"Compare logistic regression w. decision trees, neural networks. How have these technologies been vastly improved over the last 15 years?",,dezyre,,
Do you know / used data reduction techniques other than PCA? What do you think of step-wise regression? What kind of step-wise techniques are you familiar with? When is full data better than reduced data or sample?,,dezyre,,
"How would you build non parametric confidence intervals, e.g. for scores? (see the AnalyticBridge theorem)",,dezyre,,
"Are you familiar either with extreme value theory, monte carlo simulations or mathematical statistics (or anything else) to correctly estimate the chance of a very rare event?",,dezyre,,
What is root cause analysis? How to identify a cause vs. a correlation? Give examples.,,dezyre,,
How would you define and measure the predictive power of a metric?,,dezyre,,
"How to detect the best rule set for a fraud detection scoring technology? How do you deal with rule redundancy, rule discovery, and the combinatorial nature of the problem (for finding optimum rule set - the one with best predictive power)? Can an approximate solution to the rule set problem be OK? How would you find an OK approximate solution? How would you decide it is good enough and stop looking for a better one?",,dezyre,,
How to create a keyword taxonomy?,,dezyre,,
What is a Botnet? How can it be detected?,,dezyre,,
Any experience with using API's? Programming API's? Google or Amazon API's? AaaS (Analytics as a service)?,,dezyre,,
When is it better to write your own code than using a data science software package?,,dezyre,,
Which tools do you use for visualization? What do you think of Tableau? R? SAS? (for graphs). How to efficiently represent 5 dimension in a chart (or in a video)?,,dezyre,,
What is POC (proof of concept)?,,dezyre,,
"What types of clients have you been working with: internal, external, sales / finance / marketing / IT people? Consulting experience? Dealing with vendors, including vendor selection and testing?",,dezyre,,
Are you familiar with software life cycle? With IT project life cycle - from gathering requests to maintenance?,,dezyre,,
What is a cron job? ,,dezyre,,
Are you a lone coder? A production guy (developer)? Or a designer (architect)?,,dezyre,,
"Is it better to have too many false positives, or too many false negatives?",,dezyre,,
"Are you familiar with pricing optimization, price elasticity, inventory management, competitive intelligence? Give examples. ",,dezyre,,
How does Zillow's algorithm work? (to estimate the value of any home in US),,dezyre,,
"How to detect bogus reviews, or bogus Facebook accounts used for bad purposes?",,dezyre,,
How would you create a new anonymous digital currency?,,dezyre,,
Have you ever thought about creating a startup? Around which idea / concept?,,dezyre,,
Do you think that typed login / password will disappear? How could they be replaced?,,dezyre,,
Have you used time series models? Cross-correlations with time lags? Correlograms? Spectral analysis? Signal processing and filtering techniques? In which context?,,dezyre,,
Which data scientists do you admire most? which startups?,,dezyre,,
How did you become interested in data science?,,dezyre,,
"What is an efficiency curve? What are its drawbacks, and how can they be overcome?",,dezyre,,
What is a recommendation engine? How does it work?,,dezyre,,
What is an exact test? How and when can simulations help us when we do not use an exact test?,,dezyre,,
What do you think makes a good data scientist?,,dezyre,,
Do you think data science is an art or a science?,,dezyre,,
"What is the computational complexity of a good, fast clustering algorithm? What is a good clustering algorithm? How do you determine the number of clusters? How would you perform clustering on one million unique keywords, assuming you have 10 million data points - each one consisting of two keywords, and a metric measuring how similar these two keywords are? How would you create this 10 million data points table in the first place?",,dezyre,,
"Give a few examples of ""best practices"" in data science.",,dezyre,,
"What could make a chart misleading, difficult to read or interpret? What features should a useful chart have?",,dezyre,,
"Do you know a few ""rules of thumb"" used in statistical or computer science? Or in business analytics?",,dezyre,,
What are your top 5 predictions for the next 20 years?,,dezyre,,
"How do you immediately know when statistics published in an article (e.g. newspaper) are either wrong or presented to support the author's point of view, rather than correct, comprehensive factual information on a specific subject? For instance, what do you think about the official monthly unemployment statistics regularly discussed in the press? What could make them more accurate?",,dezyre,,
Testing your analytic intuition: look at these three charts. Two of them exhibit patterns. Which ones? Do you know that these charts are called scatter-plots? Are there other ways to visually represent this type of data?,,dezyre,,
"You design a robust non-parametric statistic (metric) to replace correlation or R square, that (1) is independent of sample size, (2) always between -1 and +1, and (3) based on rank statistics. How do you normalize for sample size? Write an algorithm that computes all permutations of n elements. How do you sample permutations (that is, generate tons of random permutations) when n is large, to estimate the asymptotic distribution for your newly created metric? You may use this asymptotic distribution for normalizing your metric. Do you think that an exact theoretical distribution might exist, and therefore, we should find it, and use it rather than wasting our time trying to estimate the asymptotic distribution using simulations? ",,dezyre,,
"More difficult, technical question related to previous one. There is an obvious one-to-one correspondence between permutations of n elements and integers between 1 and n! Design an algorithm that encodes an integer less than n! as a permutation of n elements. What would be the reverse algorithm, used to decode a permutation and transform it back into a number? Hint: An intermediate step is to use the factorial number system representation of an integer. Feel free to check this reference online to answer the question. Even better, feel free to browse the web to find the full answer to the question (this will test the candidate's ability to quickly search online and find a solution to a problem without spending hours reinventing the wheel).  ",,dezyre,,
"How many ""useful"" votes will a Yelp review receive? My answer: Eliminate bogus accounts (read this article), or competitor reviews (how to detect them: use taxonomy to classify users, and location - two Italian restaurants in same Zip code could badmouth each other and write great comments for themselves). Detect fake likes: some companies (e.g. FanMeNow.com) will charge you to produce fake accounts and fake likes. Eliminate prolific users who like everything, those who hate everything. Have a blacklist of keywords to filter fake reviews. See if IP address or IP block of reviewer is in a blacklist such as ""Stop Forum Spam"". Create honeypot to catch fraudsters.  Also watch out for disgruntled employees badmouthing their former employer. Watch out for 2 or 3 similar comments posted the same day by 3 users regarding a company that receives very few reviews. Is it a brand new company? Add more weight to trusted users (create a category of trusted users).  Flag all reviews that are identical (or nearly identical) and come from same IP address or same user. Create a metric to measure distance between two pieces of text (reviews). Create a review or reviewer taxonomy. Use hidden decision trees to rate or score review and reviewers.",,dezyre,,
What did you do today? Or what did you do this week / last week?,,dezyre,,
What/when is the latest data mining book / article you read? What/when is the latest data mining conference / webinar / class / workshop / training you attended? What/when is the most recent programming skill that you acquired?,,dezyre,,
"What are your favorite data science websites? Who do you admire most in the data science community, and why? Which company do you admire most?",,dezyre,,
What/when/where is the last data science blog post you wrote? ,,dezyre,,
"In your opinion, what is data science? Machine learning? Data mining?",,dezyre,,
Who are the best people you recruited and where are they today?,,dezyre,,
"What are your favorite data science websites? Who do you admire most in the data science community, and why? Which company do you admire most?",,dezyre,,
What/when/where is the last data science blog post you wrote? ,,dezyre,,
"In your opinion, what is data science? Machine learning? Data mining?",,dezyre,,
Who are the best people you recruited and where are they today?,,dezyre,,
"Can you estimate and forecast sales for any book, based on Amazon public data? Hint: read this article.",,dezyre,,
What's wrong with this picture?,,dezyre,,
"Should removing stop words be Step 1 rather than Step 3, in the search engine algorithm described here? Answer: Have you thought about the fact that mine and yours could also be stop words? So in a bad implementation, data mining would become data mine after stemming, then data. In practice, you remove stop words before stemming. So Step 3 should indeed become step 1. ",,dezyre,,
Experimental design and a bit of computer science with Lego's,,dezyre,,
You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.),"Processing a high dimensional data on a limited memory machine is a strenuous task, your interviewer would be fully aware of that. Following are the methods you can use to tackle such situation:

Since we have lower RAM, we should close all other applications in our machine, including the web browser, so that most of the memory can be put to use.
We can randomly sample the data set. This means, we can create a smaller data set, let’s say, having 1000 variables and 300000 rows and do the computations.
To reduce dimensionality, we can separate the numerical and categorical variables and remove the correlated variables. For numerical variables, we’ll use correlation. For categorical variables, we’ll use chi-square test.
Also, we can use PCA and pick the components which can explain the maximum variance in the data set.
Using online learning algorithms like Vowpal Wabbit (available in Python) is a possible option.
Building a linear model using Stochastic Gradient Descent is also helpful.
We can also apply our business understanding to estimate which all predictors can impact the response variable. But, this is an intuitive approach, failing to identify useful predictors might result in significant loss of information.",analyticsvidhya,,
" Is rotation necessary in PCA? If yes, Why? What will happen if you don’t rotate the components?","Yes, rotation (orthogonal) is necessary because it maximizes the difference between variance captured by the component. This makes the components easier to interpret. Not to forget, that’s the motive of doing PCA where, we aim to select fewer components (than features) which can explain the maximum variance in the data set. By doing rotation, the relative location of the components doesn’t change, it only changes the actual coordinates of the points.",analyticsvidhya,,
You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?,"This question has enough hints for you to start thinking! Since, the data is spread across median, let’s assume it’s a normal distribution. We know, in a normal distribution, ~68% of the data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data unaffected. Therefore, ~32% of the data would remain unaffected by missing values.",analyticsvidhya,,
You are given a data set on cancer detection. You’ve build a classification model and achieved an accuracy of 96%. Why shouldn’t you be happy with your model performance? What can you do about it?,"If you have worked on enough data sets, you should deduce that cancer detection results in imbalanced data. In an imbalanced data set, accuracy should not be used as a measure of performance because 96% (as given) might only be predicting majority class correctly, but our class of interest is minority class (4%) which is the people who actually got diagnosed with cancer. Hence, in order to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine class wise performance of the classifier. If the minority class performance is found to to be poor, we can undertake the following steps:

We can use undersampling, oversampling or SMOTE to make the data balanced.
We can alter the prediction threshold value by doing probability caliberation and finding a optimal threshold using AUC-ROC curve.
We can assign weight to classes such that the minority classes gets larger weight.
We can also use anomaly detection.",analyticsvidhya,,
Why is naive Bayes so ‘naive’ ?,"naive Bayes is so ‘naive’ because it assumes that all of the features in a data set are equally important and independent. As we know, these assumption are rarely true in real world scenario.",analyticsvidhya,,
"Explain prior probability, likelihood and marginal likelihood in context of naiveBayes algorithm?","Prior probability is nothing but, the proportion of dependent (binary) variable in the data set. It is the closest guess you can make about a class, without any further information. For example: In a data set, the dependent variable is binary (1 and 0). The proportion of 1 (spam) is 70% and 0 (not spam) is 30%. Hence, we can estimate that there are 70% chances that any new email would  be classified as spam.

Likelihood is the probability of classifying a given observation as 1 in presence of some other variable. For example: The probability that the word ‘FREE’ is used in previous spam message is likelihood. Marginal likelihood is, the probability that the word ‘FREE’ is used in any message.",analyticsvidhya,,
"You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this happen? Why?","Time series data is known to posses linearity. On the other hand, a decision tree algorithm is known to work best to detect non – linear interactions. The reason why decision tree failed to provide robust predictions because it couldn’t map the linear relationship as good as a regression model did. Therefore, we learned that, a linear regression model can provide robust prediction given the data set satisfies its linearity assumptions.",analyticsvidhya,,
"You are assigned a new project which involves helping a food delivery company save more money. The problem is, company’s delivery team aren’t able to deliver food on time. As a result, their customers get unhappy. And, to keep them happy, they end up delivering food for free. Which machine learning algorithm can save them?","You might have started hopping through the list of ML algorithms in your mind. But, wait! Such questions are asked to test your machine learning fundamentals.
This is not a machine learning problem. This is a route optimization problem. A machine learning problem consist of three things:

There exist a pattern.
You cannot solve it mathematically (even by writing exponential equations).
You have data on it.
Always look for these three factors to decide if machine learning is a tool to solve a particular problem.",analyticsvidhya,,
You came to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?,"Low bias occurs when the model’s predicted values are near to actual values. In other words, the model becomes flexible enough to mimic the training data distribution. While it sounds like great achievement, but not to forget, a flexible model has no generalization capabilities. It means, when this model is tested on an unseen data, it gives disappointing results.

In such situations, we can use bagging algorithm (like random forest) to tackle high variance problem. Bagging algorithms divides a data set into subsets made with repeated randomized sampling. Then, these samples are used to generate  a set of models using a single learning algorithm. Later, the model predictions are combined using voting (classification) or averaging (regression).

Also, to combat high variance, we can:

Use regularization technique, where higher model coefficients get penalized, hence lowering model complexity.
Use top n features from variable importance chart. May be, with all the variable in the data set, the algorithm is having difficulty in finding the meaningful signal.",analyticsvidhya,,
"You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?","Chances are, you might be tempted to say No, but that would be incorrect. Discarding correlated variables have a substantial effect on PCA because, in presence of correlated variables, the variance explained by a particular component gets inflated.

For example: You have 3 variables in a data set, of which 2 are correlated. If you run PCA on this data set, the first principal component would exhibit twice the variance than it would exhibit with uncorrelated variables. Also, adding correlated variables lets PCA put more importance on those variable, which is misleading.

 ",analyticsvidhya,,
"After spending several hours, you are now anxious to build a high accuracy model. As a result, you build 5 GBM models, thinking a boosting algorithm would do the magic. Unfortunately, neither of models could perform better than benchmark score. Finally, you decided to combine those models. Though, ensembled models are known to return high accuracy, but you are unfortunate. Where did you miss?","As we know, ensemble learners are based on the idea of combining weak learners to create strong learners. But, these learners provide superior result when the combined models are uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement, suggests that the models are correlated. The problem with correlated models is, all the models provide same information.

For example: If model 1 has classified User1122 as 1, there are high chances model 2 and model 3 would have done the same, even if its actual value is 0. Therefore, ensemble learners are built on the premise of combining weak uncorrelated models to obtain better predictions.",analyticsvidhya,,
 How is kNN different from kmeans clustering?,"Don’t get mislead by ‘k’ in their names. You should know that the fundamental difference between both these algorithms is, kmeans is unsupervised in nature and kNN is supervised in nature. kmeans is a clustering algorithm. kNN is a classification (or regression) algorithm.

kmeans algorithm partitions a data set into clusters such that a cluster formed is homogeneous and the points in each cluster are close to each other. The algorithm tries to maintain enough separability between these clusters. Due to unsupervised nature, the clusters have no labels.

kNN algorithm tries to classify an unlabeled observation based on its k (can be any number ) surrounding neighbors. It is also known as lazy learner because it involves minimal training of model. Hence, it doesn’t use training data to make generalization on unseen data set.",analyticsvidhya,,
How is True Positive Rate and Recall related? Write the equation.,"True Positive Rate = Recall. Yes, they are equal having the formula (TP/TP + FN).",analyticsvidhya,,
"You have built a multiple regression model. Your model R² isn’t as good as you wanted. For improvement, your remove the intercept term, your model R² becomes 0.8 from 0.3. Is it possible? How?","Yes, it is possible. We need to understand the significance of intercept term in a regression model. The intercept term shows model prediction without any independent variable i.e. mean prediction. The formula of R² = 1 – ∑(y – y´)²/∑(y – ymean)² where y´ is predicted value.   

When intercept term is present, R² value evaluates your model wrt. to the mean model. In absence of intercept term (ymean), the model can make no such evaluation, with large denominator, ∑(y - y´)²/∑(y)² equation’s value becomes smaller than actual, resulting in higher R².",analyticsvidhya,,
"After analyzing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if he’s true? Without losing any information, can you still build a better model?","To check multicollinearity, we can create a correlation matrix to identify & remove variables having correlation above 75% (deciding a threshold is subjective). In addition, we can use calculate VIF (variance inflation factor) to check the presence of multicollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Also, we can use tolerance as an indicator of multicollinearity.

But, removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalized regression models like ridge or lasso regression. Also, we can add some random noise in correlated variable so that the variables become different from each other. But, adding noise might affect the prediction accuracy, hence this approach should be carefully used.",analyticsvidhya,,
When is Ridge regression favorable over Lasso regression?,"You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in presence of few variables with medium / large sized effect, use lasso regression. In presence of many variables with small / medium sized effect, use ridge regression.

Conceptually, we can say, lasso regression (L1) does both variable selection and parameter shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all the coefficients in the model. In presence of correlated variables, ridge regression might be the preferred choice. Also, ridge regression works best in situations where the least square estimates have higher variance. Therefore, it depends on our model objective.",analyticsvidhya,,
Rise in global average temperature led to decrease in number of pirates around the world. Does that mean that decrease in number of pirates caused the climate change?,"After reading this question, you should have understood that this is a classic case of “causation and correlation”. No, we can’t conclude that decrease in number of pirates caused the climate change because there might be other factors (lurking or confounding variables) influencing this phenomenon.

Therefore, there might be a correlation between global average temperature and number of pirates, but based on this information we can’t say that pirated died because of rise in global average temperature.",analyticsvidhya,,
"While working on a data set, how do you select important variables? Explain your methods.","Following are the methods of variable selection you can use:

Remove the correlated variables prior to selecting important variables
Use linear regression and select variables based on p values
Use Forward Selection, Backward Selection, Stepwise Selection
Use Random Forest, Xgboost and plot variable importance chart
Use Lasso Regression
Measure information gain for the available set of features and select top n features accordingly.",analyticsvidhya,,
What is the difference between covariance and correlation?,"Correlation is the standardized form of covariance.

Covariances are difficult to compare. For example: if we calculate the covariances of salary ($) and age (years), we’ll get different covariances which can’t be compared because of having unequal scales. To combat such situation, we calculate correlation to get a value between -1 and 1, irrespective of their respective scale.",analyticsvidhya,,
"Is it possible capture the correlation between continuous and categorical variable? If yes, how?","Answer: Yes, we can use ANCOVA (analysis of covariance) technique to capture association between continuous and categorical variables.",analyticsvidhya,,
"Both being tree based algorithm, how is random forest different from Gradient boosting algorithm (GBM)?"," The fundamental difference is, random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions.

In bagging technique, a data set is divided into n samples using randomized sampling. Then, using a single learning algorithm a model is build on all samples. Later, the resultant predictions are combined using voting or averaging. Bagging is done is parallel. In boosting, after the first round of predictions, the algorithm weighs misclassified predictions higher, such that they can be corrected in the succeeding round. This sequential process of giving higher weights to misclassified predictions continue until a stopping criterion is reached.

Random forest improves model accuracy by reducing variance (mainly). The trees grown are uncorrelated to maximize the decrease in variance. On the other hand, GBM improves accuracy my reducing both bias and variance in a model.",analyticsvidhya,,
Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?,"A classification trees makes decision based on Gini Index and Node Entropy. In simple words, the tree algorithm find the best possible feature which can divide the data set into purest possible children nodes.

Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure. We can calculate Gini as following:

Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).
Calculate Gini for split using weighted Gini score of each node of that split
Entropy is the measure of impurity as given by (for binary class):

Entropy, Decision Tree

Here p and q is probability of success and failure respectively in that node. Entropy is zero when a node is homogeneous. It is maximum when a both the classes are present in a node at 50% – 50%.  Lower entropy is desirable.",analyticsvidhya,,
"You’ve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Haven’t you trained your model perfectly?","The model has overfitted. Training error 0.00 means the classifier has mimiced the training data patterns to an extent, that they are not available in the unseen data. Hence, when this classifier was run on unseen sample, it couldn’t find those patterns and returned prediction with higher error. In random forest, it happens when we use larger number of trees than necessary. Hence, to avoid these situation, we should tune number of trees using cross validation.",analyticsvidhya,,
You’ve got a data set to work having p (no. of variable) > n (no. of observation). Why is OLS as bad option to work with? Which techniques would be best to use? Why?,"In such high dimensional data sets, we can’t use classical regression techniques, since their assumptions tend to fail. When p > n, we can no longer calculate a unique least square coefficient estimate, the variances become infinite, so OLS cannot be used at all.

To combat this situation, we can use penalized regression methods like lasso, LARS, ridge which can shrink the coefficients to reduce variance. Precisely, ridge regression works best in situations where the least square estimates have higher variance.

Among other methods include subset regression, forward stepwise regression.

 ",analyticsvidhya,,
What is convex hull ? (Hint: Think SVM),"In case of linearly separable data, convex hull represents the outer boundaries of the two group of data points. Once convex hull is created, we get maximum margin hyperplane (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to create greatest separation between two groups.",analyticsvidhya,,
"We know that one hot encoding increasing the dimensionality of a data set. But, label encoding doesn’t. How ?","Don’t get baffled at this question. It’s a simple question asking the difference between the two.

Using one hot encoding, the dimensionality (a.k.a features) in a data set get increased because it creates a new variable for each level present in categorical variables. For example: let’s say we have a variable ‘color’. The variable has 3 levels namely Red, Blue and Green. One hot encoding ‘color’ variable will generate three new variables as Color.Red, Color.Blue and Color.Green containing 0 and 1 value.

In label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new variable is created. Label encoding is majorly used for binary variables.",analyticsvidhya,,
What cross validation technique would you use on time series data set? Is it k-fold or LOOCV?,"Neither.

In time series problem, k fold can be troublesome because there might be some pattern in year 4 or 5 which is not in year 3. Resampling the data set will separate these trends, and we might end up validation on past years, which is incorrect. Instead, we can use forward chaining strategy with 5 fold as shown below:

fold 1 : training [1], test [2]
fold 2 : training [1 2], test [3]
fold 3 : training [1 2 3], test [4]
fold 4 : training [1 2 3 4], test [5]
fold 5 : training [1 2 3 4 5], test [6]
where 1,2,3,4,5,6 represents “year”.",analyticsvidhya,,
"You are given a data set consisting of variables having more than 30% missing values? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?","We can deal with them in the following ways:

Assign a unique category to missing values, who knows the missing values might decipher some trend
We can remove them blatantly.
Or, we can sensibly check their distribution with the target variable, and if found any pattern we’ll keep those missing values and assign them a new category while removing others.",analyticsvidhya,,
"‘People who bought this, also bought…’ recommendations seen on amazon is a result of which algorithm?","The basic idea for this kind of recommendation engine comes from collaborative filtering.

Collaborative Filtering algorithm considers “User Behavior” for recommending items. They exploit behavior of other users and items in terms of transaction history, ratings, selection and purchase information. Other users behaviour and preferences over the items are used to recommend items to the new users. In this case, features of the items are not known.",analyticsvidhya,,
What do you understand by Type I vs Type II error ?,"Type I error is committed when the null hypothesis is true and we reject it, also known as a ‘False Positive’. Type II error is committed when the null hypothesis is false and we accept it, also known as ‘False Negative’.

In the context of confusion matrix, we can say Type I error occurs when we classify a value as positive (1) when it is actually negative (0). Type II error occurs when we classify a value as negative (0) when it is actually positive(1).",analyticsvidhya,,
"You are working on a classification problem. For validation purposes, you’ve randomly sampled the training data set into train and validation. You are confident that your model will work incredibly well on unseen data since your validation accuracy is high. However, you get shocked after getting poor test accuracy. What went wrong?","In case of classification problem, we should always use stratified sampling instead of random sampling. A random sampling doesn’t takes into consideration the proportion of target classes. On the contrary, stratified sampling helps to maintain the distribution of target variable in the resultant distributed samples also.

 ",analyticsvidhya,,
"You have been asked to evaluate a regression model based on R², adjusted R² and tolerance. What will be your criteria?","Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of percent of variance in a predictor which cannot be accounted by other predictors. Large values of tolerance is desirable.

We will consider adjusted R² as opposed to R² to evaluate model fit because R² increases irrespective of improvement in prediction accuracy as we add more variables. But, adjusted R² would only increase if an additional variable improves the accuracy of model, otherwise stays same. It is difficult to commit a general threshold value for adjusted R² because it varies between data sets. For example: a gene mutation data set might result in lower adjusted R² and still provide fairly good predictions, as compared to a stock market data where lower adjusted R² implies that model is not good.",analyticsvidhya,,
"In k-means or kNN, we use euclidean distance to calculate the distance between nearest neighbors. Why not manhattan distance ?","We don’t use manhattan distance because it calculates distance horizontally or vertically only. It has dimension restrictions. On the other hand, euclidean metric can be used in any space to calculate distance. Since, the data points can be present in any dimension, euclidean distance is a more viable option.

Example: Think of a chess board, the movement made by a bishop or a rook is calculated by manhattan distance because of their respective vertical & horizontal movements.",analyticsvidhya,,
Explain machine learning to me like a 5 year old.,"It’s simple. It’s just like how babies learn to walk. Every time they fall down, they learn (unconsciously) & realize that their legs should be straight and not in a bend position. The next time they fall down, they feel pain. They cry. But, they learn ‘not to stand like that again’. In order to avoid that pain, they try harder. To succeed, they even seek support from the door or wall or anything near them, which helps them stand firm.

This is how a machine works & develops intuition from its environment.",analyticsvidhya,,
 I know that a linear regression model is generally evaluated using Adjusted R² or F value. How would you evaluate a logistic regression model?,"We can use the following methods:

Since logistic regression is used to predict probabilities, we can use AUC-ROC curve along with confusion matrix to determine its performance.
Also, the analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.
Null Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model. Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model.",analyticsvidhya,,
"Considering the long list of machine learning algorithm, given a data set, how do you decide which one to use?","You should say, the choice of machine learning algorithm solely depends of the type of data. If you are given a data set which is exhibits linearity, then linear regression would be the best algorithm to use. If you given to work on images, audios, then neural network would help you to build a robust model.

If the data comprises of non linear interactions, then a boosting or bagging algorithm should be the choice. If the business requirement is to build a model which can be deployed, then we’ll use regression or a decision tree model (easy to interpret and explain) instead of black box algorithms like SVM, GBM etc.

In short, there is no one master algorithm for all situations. We must be scrupulous enough to understand which algorithm to use.",analyticsvidhya,,
Do you suggest that treating a categorical variable as continuous variable would result in a better predictive model?,"For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature.",analyticsvidhya,,
When does regularization becomes necessary in Machine Learning?,"Regularization becomes necessary when the model begins to ovefit / underfit. This technique introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term. This helps to reduce model complexity so that the model can become better at predicting (generalizing).",analyticsvidhya,,
What do you understand by Bias Variance trade off?,"The error emerging from any model can be broken down into three components mathematically. Following are these component :

error of a model

Bias error is useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have a under-performing model which keeps on missing important trends. Variance on the other side quantifies how are the prediction made on same observation different from each other. A high variance model will over-fit on your training population and perform badly on any observation beyond training.

 ",analyticsvidhya,,
OLS is to linear regression. Maximum likelihood is to logistic regression. Explain the statement.,"OLS and Maximum likelihood are the methods used by the respective regression methods to approximate the unknown parameter (coefficient) value. In simple words,

Ordinary least square(OLS) is a method used in linear regression which approximates the parameters resulting in minimum distance between actual and predicted values. Maximum Likelihood helps in choosing the the values of parameters which maximizes the likelihood that the parameters are most likely to produce observed data.",analyticsvidhya,,