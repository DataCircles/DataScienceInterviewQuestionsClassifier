question,answer,source,type,track
"Tell me about a recent data analysis project you worked on?  
What were the interesting or unexpected findings that you had from the project?
How did you end up productionizing it if at all or what was the end result?",,,Experience,General
"Suppose we offered 3 promotions at the Google Play Store.
How would we know if the promotions were successful or not?",,,Case Study,Quantitative
Describe different approaches to ranking photos at a given location and evaluating these ranking algorithms.,,,Case Study,Quantitative
"Suppose you want to calculate the probability that game 7 is reached.  How would you use Monte Carlo simulation to estimate this probability and using transition matrices?  You have two teams A/B with the following probabilities:

Pr(A wins at time t=1) = p1
Pr(A wins at time t=k | A wins at time t=k-1) = p11
Pr(A wins at time t=k | B wins at time t=k-1) = p12
Pr(B wins at time t=k | A wins at time t=k-1) = p21
Pr(B wins at time t=k | B wins at time t=k-1) = p22
",,,Programming,Quantitative
"People are submitting contradictory edits for submissions to different places for a maps app where some say it is open and others say it is closed.
How would you assess the reliability of the submissions?",,,Case Study,Quantitative
What are different types of models used for analyzing and evaluating the causal impacts of different product or marketing strategies?,,,Knowledge,Quantitative
What is the functional form for a maximum likelihood estimation of a logit regression model?,,,Knowledge,Quantitative
What are the assumptions required for linear regression?,"There are four major assumptions: 1. There is a linear relationship between the dependent variables and the regressors, meaning the model you are creating actually fits the data, 2. The errors or residuals of the data are normally distributed and independent from each other, 3. There is minimal multicollinearity between explanatory variables, and 4. Homoscedasticity. This means the variance around the regression line is the same for all values of the predictor variable.",springboard,,
What is the Central Limit Theorem and why is it important?,"Suppose that we are interested in estimating the average height among all people. Collecting data for every person in the world is impossible. While we can’t obtain a height measurement from everyone in the population, we can still sample some people. The question now becomes, what can we say about the average height of the entire population given a single sample. The Central Limit Theorem addresses this question exactly.",springboard,,
What is sampling? How many sampling methods do you know?,"Data sampling is a statistical analysis technique used to select, manipulate and analyze a representative subset of data points to identify patterns and trends in the larger data set being examined.”",springboard,,
What is the difference between type I vs type II error?,"A type I error occurs when the null hypothesis is true, but is rejected. A type II error occurs when the null hypothesis is false, but erroneously fails to be rejected.",springboard,,
"What is linear regression? What do the terms p-value, coefficient, and r-squared value mean? What is the significance of each of these components?","A linear regression is a good tool for quick predictive analysis: for example, the price of a house depends on a myriad of factors, such as its size or its location. In order to see the relationship between these variables, we need to build a linear regression, which predicts the line of best fit between them and can help conclude whether or not these two factors have a positive or negative relationship",springboard,,
What are the assumptions required for linear regression?,"There are four major assumptions: 1. There is a linear relationship between the dependent variables and the regressors, meaning the model you are creating actually fits the data, 2. The errors or residuals of the data are normally distributed and independent from each other, 3. There is minimal multicollinearity between explanatory variables, and 4. Homoscedasticity. This means the variance around the regression line is the same for all values of the predictor variable.",springboard,,
What is a statistical interaction?,"”Basically, an interaction is when the effect of one factor (input variable) on the dependent variable (output variable) differs among levels of another factor.”",springboard,,
What is selection bias?,"“Selection (or ‘sampling’) bias occurs in an ‘active,’ sense when the sample data that is gathered and prepared for modeling has characteristics that are not representative of the true, future population of cases the model will see. That is, active selection bias occurs when a subset of the data are systematically (i.e., non-randomly) excluded from analysis.”",springboard,,
What is an example of a data set with a non-Gaussian distribution?,"“The Gaussian distribution is part of the Exponential family of distributions, but there are a lot more of them, with the same sort of ease of use, in many cases, and if the person doing the machine learning has a solid grounding in statistics, they can be utilized where appropriate.” ",springboard,,
What is the Binomial Probability Formula?,“The binomial distribution consists of the probabilities of each of the possible numbers of successes on N trials for independent events that each have a probability of π (the Greek letter pi) of occurring.”,springboard,,
With which programming languages and environments are you most comfortable working?,,springboard,,
What are some pros and cons about your favorite statistical software?,,springboard,,
Tell me about an original algorithm you’ve created.,,springboard,,
Describe a data science project in which you worked with a substantial programming component. What did you learn from that experience?,,springboard,,
Do you contribute to any open-source projects?,,springboard,,
How would you clean a data set in (insert language here)?,,springboard,,
Tell me about the coding you did during your last project?,,springboard,,
What are two main components of the Hadoop framework?,"The Hadoop Distributed File System (HDFS), MapReduce, and YARN.",springboard,,
Explain how MapReduce works as simply as possible.,MapReduce is a programming model that enables distributed processing of large data sets on compute clusters of commodity hardware. Hadoop MapReduce first performs mapping which involves splitting a large file into pieces to make another set of data.,springboard,,
How would you sort a large list of numbers?,,springboard,,
Say you’re given a large data set. What would be your plan for dealing with outliers? How about missing values? How about transformations?,,springboard,,
What modules/libraries are you most familiar with? What do you like or dislike about them?,,springboard,,
"In Python, how is memory managed?",,springboard,,
"In Python, memory is managed in a private heap space. This means that all the objects and data structures will be located in a private heap. However, the programmer won’t be allowed to access this heap. Instead, the Python interpreter will handle it. At the same time, the core API will enable access to some Python tools for the programmer to start coding. The memory manager will allocate the heap space for the Python objects while the inbuilt garbage collector will recycle all the memory that’s not being used to boost available heap space. ",,springboard,,
What are the supported data types in Python?,,springboard,,
"“Python’s built-in (or standard) data types can be grouped into several classes. Sticking to the hierarchy scheme used in the official Python documentation these are numeric types, sequences, sets and mappings.”",,springboard,,
What is the difference between a tuple and a list in Python?,“Apart from tuples being immutable there is also a semantic distinction that should guide their usage.”,springboard,,
What are the different types of sorting algorithms available in R language?,"There are insertion, bubble, and selection sorting algorithms.",springboard,,
What are the different data objects in R?,"“R objects can store values as different core data types (referred to as modes in R jargon); these include numeric (both integer and double), character and logical.” ",springboard,,
What packages are you most familiar with? What do you like or dislike about them?,,springboard,,
How do you access the element in the 2nd column and 4th row of a matrix named M?,"We can access elements of a matrix using the square bracket [ indexing method. Elements can be accessed as var[row, column]",springboard,,
What is the command used to store R objects in a file?,,springboard,,
What is the best way to use Hadoop and R together for analysis?,“Hadoop and R complement each other quite well in terms of visualization and analytics of big data. There are four different ways of using Hadoop and R together.”,springboard,,
How do you split a continuous variable into different groups/ranks in R?,,springboard,,
Write a function in R language to replace the missing value in a vector with the mean of that vector.,,springboard,,
What is the purpose of the group functions in SQL? Give some examples of group functions.,"Group functions are necessary to get summary statistics of a data set. COUNT, MAX, MIN, AVG, SUM, and DISTINCT are all group functions.",springboard,,
"Tell me the difference between an inner join, left join/right join, and union.","“In a Venn diagram the inner join is when both tables have a match, a left join is when there is a match in the left table and the right table is null, a right join is the opposite of a left join, and a full join is all of the data combined.”",springboard,,
What does UNION do? What is the difference between UNION and UNION ALL?,"“UNION removes duplicate records (where all columns in the results are the same), UNION ALL does not.” ",springboard,,
What is the difference between SQL and MySQL or SQL Server?,"SQL stands for Structured Query Language. It’s a standard language for accessing and manipulating databases. MySQL is a database management system, like SQL Server, Oracle, Informix, Postgres, etc.",springboard,,
"If a table contains duplicate rows, does a query result display the duplicate values by default? How can you eliminate duplicate rows from a query result?",Yes. One way you can eliminate duplicate rows with the DISTINCT clause.,springboard,,
Tell me about how you designed a model for a past employer or client.,,springboard,,
What are your favorite data visualization techniques?,,springboard,,
How would you effectively represent data with 5 dimensions?,,springboard,,
How is k-NN different from k-means clustering?,"k-NN, or k-nearest neighbors is a classification algorithm, where the k is an integer describing the number of neighboring data points that influence the classification of a given observation. K-means is a clustering algorithm, where the k is an integer describing the number of clusters to be created from the given data.",springboard,,
How would you create a logistic regression model?,,springboard,,
Have you used a time series model? Do you understand cross-correlations with time lags?,,springboard,,
"Explain the 80/20 rule, and tell me about its importance in model validation.",“People usually tend to start with a 80-20% split (80% training set – 20% test set) and split the training set once more into a 80-20% ratio to create the validation set.”,springboard,,
Explain what precision and recall are. How do they relate to the ROC curve?,"Recall describes what percentage of true positives are described as positive by the model. Precision describes what percent of positive predictions were correct. The ROC curve shows the relationship between model recall and specificity–specificity being a measure of the percent of true negatives being described as negative by the model. Recall, precision, and the ROC are measures used to identify how useful a given classification model is. Read more here.",springboard,,
Explain the difference between L1 and L2 regularization methods.,“A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term.” ,springboard,,
What is root cause analysis?,"All of us dread that meeting where the boss asks ‘why is revenue down?’ The only thing worse than that question is not having any answers! There are many changes happening in your business every day, and often you will want to understand exactly what is driving a given change — especially if it is unexpected. Understanding the underlying causes of change is known as root cause analysis.",springboard,,
What are hash table collisions?,"If the range of key values is larger than the size of our hash table, which is usually always the case, then we must account for the possibility that two different records with two different keys can hash to the same table index. There are a few different ways to resolve this issue. In hash table vernacular, this solution implemented is referred to as collision resolution.",springboard,,
What is an exact test?,"In statistics, an exact (significance) test is a test where all assumptions, upon which the derivation of the distribution of the test statistic is based, are met as opposed to an approximate test (in which the approximation may be made as close as desired by making the sample size big enough). This will result in a significance test that will have a false rejection rate always equal to the significance level of the test. For example an exact test at significance level 5% will in the long run reject true null hypotheses exactly 5% of the time.",springboard,,
"In your opinion, which is more important when designing a machine learning model: model performance or model accuracy?","What is one way that you would handle an imbalanced data set that’s being used for prediction (i.e., vastly more negative classes than positive classes)?",springboard,,
How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression?,,springboard,,
I have two models of comparable accuracy and computational performance. Which one should I choose for production and why?,,springboard,,
How do you deal with sparsity?,,springboard,,
Is it better to spend five days developing a 90-percent accurate solution or 10 days for 100-percent accuracy?,,springboard,,
What are some situations where a general linear model fails?,,springboard,,
Do you think 50 small decision trees are better than a large one? Why?,,springboard,,
"When modifying an algorithm, how do you know that your changes are an improvement over not doing anything?",,springboard,,
Is it better to have too many false positives or too many false negatives?,,springboard,,
Tell me about a time when you took initiative.,,springboard,,
Tell me about a time when you had to overcome a dilemma.,,springboard,,
Tell me about a time when you resolved a conflict.,,springboard,,
Tell me about a time you failed and what you have learned from it.,,springboard,,
Tell me about (a job on your resume). Why did you choose to do it and what do you like most about it?,,springboard,,
Tell me about a challenge you have overcome while working on a group project.,,springboard,,
"When you encountered a tedious, boring task, how would you deal with it and motivate yourself to complete it?",,springboard,,
What have you done in the past to make a client satisfied/happy?,,springboard,,
What have you done in your previous job that you are really proud of?,,springboard,,
What do you do when your personal life is running over into your work life?,,springboard,,
Which data scientists do you admire most? Which startups?,There are plenty of amazing data scientists to choose from—take a look at this article on top data science influencers for interesting information about some of the top data scientists in the world.,springboard,,
What do you think makes a good data scientist?,,springboard,,
How did you become interested in data science?,,springboard,,
Give a few examples of “best practices” in data science.,,springboard,,
What is the latest data science book / article you read? What is the latest data mining conference / webinar / class / workshop / training you attended?,,springboard,,
What’s a project you would want to work on at our company?,,springboard,,
What unique skills do you think you’d bring to the team?,,springboard,,
What data would you love to acquire if there were no limitations?,,springboard,,
Have you ever thought about creating your own startup? Around which idea / concept?,,springboard,,
What can your hobbies tell me that your resume can’t?,,springboard,,
What are your top 5 predictions for the next 20 years?,,springboard,,
What did you do today? Or what did you do this week / last week?,,springboard,,
How would you come up with a solution to identify plagiarism?,,springboard,,
How many “useful” votes will a Yelp review receive?,,springboard,,
How do you detect individual paid accounts shared by multiple users?,,springboard,,
You are about to send a million emails. How do you optimize delivery? How do you optimize response?,,springboard,,
"You have a data set containing 100,000 rows and 100 columns, with one of those columns being our dependent variable for a problem we’d like to solve. How can we quickly identify which columns will be helpful in predicting the dependent variable. Identify two techniques and explain them to me as though I were 5 years old.",,springboard,,
"How would you detect bogus reviews, or bogus Facebook accounts used for bad purposes?","This is an opportunity to showcase your knowledge of machine learning algorithms; specifically, sentiment analysis and text analysis algorithms. Showcase your knowledge of fraudulent behavior—what are the abnormal behaviors that can typically be seen from fraudulent accounts?",springboard,,
"How would you perform clustering on a million unique keywords, assuming you have 10 million data points—each one consisting of two keywords, and a metric measuring how similar these two keywords are? How would you create this 10 million data points table in the first place?",,springboard,,
"How would you optimize a web crawler to run much faster, extract better information, and better summarize data to produce cleaner databases?",,springboard,,
,,,,
,,,,
,,,,
"How would you test if survey responses were filled at random by certain individuals, as opposed to truthful selections?",,udacity,,
What do you think a data scientist is/does?,,udacity,,
What do you think are the most important skills for a data scientist to have?,,udacity,,
"Which machine learning model (classification vs. regression, for example) to use given a particular problem.",,udacity,,
"How to go about training, testing, and validating results. Different ways of controlling for model complexity.",,udacity,,
"How to model a quantity that you can’t directly observe (using Bayesian approaches, for example, and when doing so, how to choose prior distributions).",,udacity,,
"What are various numerical optimization techniques (maximum likelihood, maximum a posteriori).",,udacity,,
"What types of data are important for a particular set of business needs, how you would go about collecting that data.",,udacity,,
"Dealing with correlated features in your data set, how to reduce the dimensionality of data.",,udacity,,
Linear/polynomial regression,,udacity,,
Decision trees,,udacity,,
Dimensionality reduction,,udacity,,
Clustering,,udacity,,
What’s a project you would want to work on at our company?,,udacity,,
What data would you go after to start working on it?,,udacity,,
What unique skills do you think you’d bring to the team?,,udacity,,
how do you detect outliers?,,dezyre,,
"What is the biggest data set that you processed, and how did you process it, what were the results?",,dezyre,,
Tell me two success stories about your analytic or computer science projects? How was lift (or success) measured?,,dezyre,,
"What is: lift, KPI, robustness, model fitting, design of experiments, 80/20 rule?",,dezyre,,
"What is: collaborative filtering, n-grams, map reduce, cosine distance?",,dezyre,,
"How to optimize a web crawler to run much faster, extract better information, and better summarize data to produce cleaner databases?",,dezyre,,
How would you come up with a solution to identify plagiarism?,,dezyre,,
How to detect individual paid accounts shared by multiple users?,,dezyre,,
Should click data be handled in real time? Why? In which contexts?,,dezyre,,
"What is better: good data or good models? And how do you define ""good""? Is there a universal good model? Are there any models that are definitely not so good?",,dezyre,,
What is probabilistic merging (AKA fuzzy merging)? Is it easier to handle with SQL or other languages? Which languages would you choose for semi-structured text data reconciliation? ,,dezyre,,
How do you handle missing data? What imputation techniques do you recommend?,,dezyre,,
What is your favorite programming language / vendor? why?,,dezyre,,
Tell me 3 things positive and 3 things negative about your favorite statistical software.,,dezyre,,
"Compare SAS, R, Python, Perl",,dezyre,,
What is the curse of big data?,,dezyre,,
Have you been involved in database design and data modeling?,,dezyre,,
Have you been involved in dashboard creation and metric selection? What do you think about Birt?,,dezyre,,
What features of Teradata do you like?,,dezyre,,
You are about to send one million email (marketing campaign). How do you optimze delivery? How do you optimize response? Can you optimize both separately? (answer: not really),,dezyre,,
"Toad or Brio or any other similar clients are quite inefficient to query Oracle databases. Why? How would you do to increase speed by a factor 10, and be able to handle far bigger outputs? ",,dezyre,,
How would you turn unstructured data into structured data? Is it really necessary? Is it OK to store data as flat text files rather than in an SQL-powered RDBMS?,,dezyre,,
What are hash table collisions? How is it avoided? How frequently does it happen?,,dezyre,,
How to make sure a mapreduce application has good load balance? What is load balance?,,dezyre,,
Examples where mapreduce does not work? Examples where it works very well? What are the security issues involved with the cloud? What do you think of EMC's solution offering an hybrid approach - both internal and external cloud - to mitigate the risks and offer other advantages (which ones)?,,dezyre,,
"Is it better to have 100 small hash tables or one big hash table, in memory, in terms of access speed (assuming both fit within RAM)? What do you think about in-database analytics?",,dezyre,,
Why is naive Bayes so bad? How would you improve a spam detection algorithm that uses naive Bayes?,,dezyre,,
Have you been working with white lists? Positive rules? (In the context of fraud or spam detection),,dezyre,,
What is star schema? Lookup tables? ,,dezyre,,
"Can you perform logistic regression with Excel? (yes) How? (use linest on log-transformed data)? Would the result be good? (Excel has numerical issues, but it's very interactive)",,dezyre,,
"Have you optimized code or algorithms for speed: in SQL, Perl, C++, Python etc. How, and by how much?",,dezyre,,
"Is it better to spend 5 days developing a 90% accurate solution, or 10 days for 100% accuracy? Depends on the context?",,dezyre,,
"Define: quality assurance, six sigma, design of experiments. Give examples of good and bad designs of experiments.",,dezyre,,
"What are the drawbacks of general linear model? Are you familiar with alternatives (Lasso, ridge regression, boosted trees)?",,dezyre,,
Do you think 50 small decision trees are better than a large one? Why?,,dezyre,,
"Is actuarial science not a branch of statistics (survival analysis)? If not, how so?",,dezyre,,
"Give examples of data that does not have a Gaussian distribution, nor log-normal. Give examples of data that has a very chaotic distribution?",,dezyre,,
Why is mean square error a bad measure of model performance? What would you suggest instead?,,dezyre,,
How can you prove that one improvement you've brought to an algorithm is really an improvement over not doing anything? Are you familiar with A/B testing?,,dezyre,,
"What is sensitivity analysis? Is it better to have low sensitivity (that is, great robustness) and low predictive power, or the other way around? How to perform good cross-validation? What do you think about the idea of injecting noise in your data set to test the sensitivity of your models?",,dezyre,,
"Compare logistic regression w. decision trees, neural networks. How have these technologies been vastly improved over the last 15 years?",,dezyre,,
Do you know / used data reduction techniques other than PCA? What do you think of step-wise regression? What kind of step-wise techniques are you familiar with? When is full data better than reduced data or sample?,,dezyre,,
"How would you build non parametric confidence intervals, e.g. for scores? (see the AnalyticBridge theorem)",,dezyre,,
"Are you familiar either with extreme value theory, monte carlo simulations or mathematical statistics (or anything else) to correctly estimate the chance of a very rare event?",,dezyre,,
What is root cause analysis? How to identify a cause vs. a correlation? Give examples.,,dezyre,,
How would you define and measure the predictive power of a metric?,,dezyre,,
"How to detect the best rule set for a fraud detection scoring technology? How do you deal with rule redundancy, rule discovery, and the combinatorial nature of the problem (for finding optimum rule set - the one with best predictive power)? Can an approximate solution to the rule set problem be OK? How would you find an OK approximate solution? How would you decide it is good enough and stop looking for a better one?",,dezyre,,
How to create a keyword taxonomy?,,dezyre,,
What is a Botnet? How can it be detected?,,dezyre,,
Any experience with using API's? Programming API's? Google or Amazon API's? AaaS (Analytics as a service)?,,dezyre,,
When is it better to write your own code than using a data science software package?,,dezyre,,
Which tools do you use for visualization? What do you think of Tableau? R? SAS? (for graphs). How to efficiently represent 5 dimension in a chart (or in a video)?,,dezyre,,
What is POC (proof of concept)?,,dezyre,,
"What types of clients have you been working with: internal, external, sales / finance / marketing / IT people? Consulting experience? Dealing with vendors, including vendor selection and testing?",,dezyre,,
Are you familiar with software life cycle? With IT project life cycle - from gathering requests to maintenance?,,dezyre,,
What is a cron job? ,,dezyre,,
Are you a lone coder? A production guy (developer)? Or a designer (architect)?,,dezyre,,
"Is it better to have too many false positives, or too many false negatives?",,dezyre,,
"Are you familiar with pricing optimization, price elasticity, inventory management, competitive intelligence? Give examples. ",,dezyre,,
How does Zillow's algorithm work? (to estimate the value of any home in US),,dezyre,,
"How to detect bogus reviews, or bogus Facebook accounts used for bad purposes?",,dezyre,,
How would you create a new anonymous digital currency?,,dezyre,,
Have you ever thought about creating a startup? Around which idea / concept?,,dezyre,,
Do you think that typed login / password will disappear? How could they be replaced?,,dezyre,,
Have you used time series models? Cross-correlations with time lags? Correlograms? Spectral analysis? Signal processing and filtering techniques? In which context?,,dezyre,,
Which data scientists do you admire most? which startups?,,dezyre,,
How did you become interested in data science?,,dezyre,,
"What is an efficiency curve? What are its drawbacks, and how can they be overcome?",,dezyre,,
What is a recommendation engine? How does it work?,,dezyre,,
What is an exact test? How and when can simulations help us when we do not use an exact test?,,dezyre,,
What do you think makes a good data scientist?,,dezyre,,
Do you think data science is an art or a science?,,dezyre,,
"What is the computational complexity of a good, fast clustering algorithm? What is a good clustering algorithm? How do you determine the number of clusters? How would you perform clustering on one million unique keywords, assuming you have 10 million data points - each one consisting of two keywords, and a metric measuring how similar these two keywords are? How would you create this 10 million data points table in the first place?",,dezyre,,
"Give a few examples of ""best practices"" in data science.",,dezyre,,
"What could make a chart misleading, difficult to read or interpret? What features should a useful chart have?",,dezyre,,
"Do you know a few ""rules of thumb"" used in statistical or computer science? Or in business analytics?",,dezyre,,
What are your top 5 predictions for the next 20 years?,,dezyre,,
"How do you immediately know when statistics published in an article (e.g. newspaper) are either wrong or presented to support the author's point of view, rather than correct, comprehensive factual information on a specific subject? For instance, what do you think about the official monthly unemployment statistics regularly discussed in the press? What could make them more accurate?",,dezyre,,
Testing your analytic intuition: look at these three charts. Two of them exhibit patterns. Which ones? Do you know that these charts are called scatter-plots? Are there other ways to visually represent this type of data?,,dezyre,,
"You design a robust non-parametric statistic (metric) to replace correlation or R square, that (1) is independent of sample size, (2) always between -1 and +1, and (3) based on rank statistics. How do you normalize for sample size? Write an algorithm that computes all permutations of n elements. How do you sample permutations (that is, generate tons of random permutations) when n is large, to estimate the asymptotic distribution for your newly created metric? You may use this asymptotic distribution for normalizing your metric. Do you think that an exact theoretical distribution might exist, and therefore, we should find it, and use it rather than wasting our time trying to estimate the asymptotic distribution using simulations? ",,dezyre,,
"More difficult, technical question related to previous one. There is an obvious one-to-one correspondence between permutations of n elements and integers between 1 and n! Design an algorithm that encodes an integer less than n! as a permutation of n elements. What would be the reverse algorithm, used to decode a permutation and transform it back into a number? Hint: An intermediate step is to use the factorial number system representation of an integer. Feel free to check this reference online to answer the question. Even better, feel free to browse the web to find the full answer to the question (this will test the candidate's ability to quickly search online and find a solution to a problem without spending hours reinventing the wheel).  ",,dezyre,,
"How many ""useful"" votes will a Yelp review receive? My answer: Eliminate bogus accounts (read this article), or competitor reviews (how to detect them: use taxonomy to classify users, and location - two Italian restaurants in same Zip code could badmouth each other and write great comments for themselves). Detect fake likes: some companies (e.g. FanMeNow.com) will charge you to produce fake accounts and fake likes. Eliminate prolific users who like everything, those who hate everything. Have a blacklist of keywords to filter fake reviews. See if IP address or IP block of reviewer is in a blacklist such as ""Stop Forum Spam"". Create honeypot to catch fraudsters.  Also watch out for disgruntled employees badmouthing their former employer. Watch out for 2 or 3 similar comments posted the same day by 3 users regarding a company that receives very few reviews. Is it a brand new company? Add more weight to trusted users (create a category of trusted users).  Flag all reviews that are identical (or nearly identical) and come from same IP address or same user. Create a metric to measure distance between two pieces of text (reviews). Create a review or reviewer taxonomy. Use hidden decision trees to rate or score review and reviewers.",,dezyre,,
What did you do today? Or what did you do this week / last week?,,dezyre,,
What/when is the latest data mining book / article you read? What/when is the latest data mining conference / webinar / class / workshop / training you attended? What/when is the most recent programming skill that you acquired?,,dezyre,,
"What are your favorite data science websites? Who do you admire most in the data science community, and why? Which company do you admire most?",,dezyre,,
What/when/where is the last data science blog post you wrote? ,,dezyre,,
"In your opinion, what is data science? Machine learning? Data mining?",,dezyre,,
Who are the best people you recruited and where are they today?,,dezyre,,
"What are your favorite data science websites? Who do you admire most in the data science community, and why? Which company do you admire most?",,dezyre,,
What/when/where is the last data science blog post you wrote? ,,dezyre,,
"In your opinion, what is data science? Machine learning? Data mining?",,dezyre,,
Who are the best people you recruited and where are they today?,,dezyre,,
"Can you estimate and forecast sales for any book, based on Amazon public data? Hint: read this article.",,dezyre,,
What's wrong with this picture?,,dezyre,,
"Should removing stop words be Step 1 rather than Step 3, in the search engine algorithm described here? Answer: Have you thought about the fact that mine and yours could also be stop words? So in a bad implementation, data mining would become data mine after stemming, then data. In practice, you remove stop words before stemming. So Step 3 should indeed become step 1. ",,dezyre,,
Experimental design and a bit of computer science with Lego's,,dezyre,,
You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.),"Processing a high dimensional data on a limited memory machine is a strenuous task, your interviewer would be fully aware of that. Following are the methods you can use to tackle such situation:

Since we have lower RAM, we should close all other applications in our machine, including the web browser, so that most of the memory can be put to use.
We can randomly sample the data set. This means, we can create a smaller data set, let’s say, having 1000 variables and 300000 rows and do the computations.
To reduce dimensionality, we can separate the numerical and categorical variables and remove the correlated variables. For numerical variables, we’ll use correlation. For categorical variables, we’ll use chi-square test.
Also, we can use PCA and pick the components which can explain the maximum variance in the data set.
Using online learning algorithms like Vowpal Wabbit (available in Python) is a possible option.
Building a linear model using Stochastic Gradient Descent is also helpful.
We can also apply our business understanding to estimate which all predictors can impact the response variable. But, this is an intuitive approach, failing to identify useful predictors might result in significant loss of information.",analyticsvidhya,,
" Is rotation necessary in PCA? If yes, Why? What will happen if you don’t rotate the components?","Yes, rotation (orthogonal) is necessary because it maximizes the difference between variance captured by the component. This makes the components easier to interpret. Not to forget, that’s the motive of doing PCA where, we aim to select fewer components (than features) which can explain the maximum variance in the data set. By doing rotation, the relative location of the components doesn’t change, it only changes the actual coordinates of the points.",analyticsvidhya,,
You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?,"This question has enough hints for you to start thinking! Since, the data is spread across median, let’s assume it’s a normal distribution. We know, in a normal distribution, ~68% of the data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data unaffected. Therefore, ~32% of the data would remain unaffected by missing values.",analyticsvidhya,,
You are given a data set on cancer detection. You’ve build a classification model and achieved an accuracy of 96%. Why shouldn’t you be happy with your model performance? What can you do about it?,"If you have worked on enough data sets, you should deduce that cancer detection results in imbalanced data. In an imbalanced data set, accuracy should not be used as a measure of performance because 96% (as given) might only be predicting majority class correctly, but our class of interest is minority class (4%) which is the people who actually got diagnosed with cancer. Hence, in order to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine class wise performance of the classifier. If the minority class performance is found to to be poor, we can undertake the following steps:

We can use undersampling, oversampling or SMOTE to make the data balanced.
We can alter the prediction threshold value by doing probability caliberation and finding a optimal threshold using AUC-ROC curve.
We can assign weight to classes such that the minority classes gets larger weight.
We can also use anomaly detection.",analyticsvidhya,,
Why is naive Bayes so ‘naive’ ?,"naive Bayes is so ‘naive’ because it assumes that all of the features in a data set are equally important and independent. As we know, these assumption are rarely true in real world scenario.",analyticsvidhya,,
"Explain prior probability, likelihood and marginal likelihood in context of naiveBayes algorithm?","Prior probability is nothing but, the proportion of dependent (binary) variable in the data set. It is the closest guess you can make about a class, without any further information. For example: In a data set, the dependent variable is binary (1 and 0). The proportion of 1 (spam) is 70% and 0 (not spam) is 30%. Hence, we can estimate that there are 70% chances that any new email would  be classified as spam.

Likelihood is the probability of classifying a given observation as 1 in presence of some other variable. For example: The probability that the word ‘FREE’ is used in previous spam message is likelihood. Marginal likelihood is, the probability that the word ‘FREE’ is used in any message.",analyticsvidhya,,
"You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this happen? Why?","Time series data is known to posses linearity. On the other hand, a decision tree algorithm is known to work best to detect non – linear interactions. The reason why decision tree failed to provide robust predictions because it couldn’t map the linear relationship as good as a regression model did. Therefore, we learned that, a linear regression model can provide robust prediction given the data set satisfies its linearity assumptions.",analyticsvidhya,,
"You are assigned a new project which involves helping a food delivery company save more money. The problem is, company’s delivery team aren’t able to deliver food on time. As a result, their customers get unhappy. And, to keep them happy, they end up delivering food for free. Which machine learning algorithm can save them?","You might have started hopping through the list of ML algorithms in your mind. But, wait! Such questions are asked to test your machine learning fundamentals.
This is not a machine learning problem. This is a route optimization problem. A machine learning problem consist of three things:

There exist a pattern.
You cannot solve it mathematically (even by writing exponential equations).
You have data on it.
Always look for these three factors to decide if machine learning is a tool to solve a particular problem.",analyticsvidhya,,
You came to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?,"Low bias occurs when the model’s predicted values are near to actual values. In other words, the model becomes flexible enough to mimic the training data distribution. While it sounds like great achievement, but not to forget, a flexible model has no generalization capabilities. It means, when this model is tested on an unseen data, it gives disappointing results.

In such situations, we can use bagging algorithm (like random forest) to tackle high variance problem. Bagging algorithms divides a data set into subsets made with repeated randomized sampling. Then, these samples are used to generate  a set of models using a single learning algorithm. Later, the model predictions are combined using voting (classification) or averaging (regression).

Also, to combat high variance, we can:

Use regularization technique, where higher model coefficients get penalized, hence lowering model complexity.
Use top n features from variable importance chart. May be, with all the variable in the data set, the algorithm is having difficulty in finding the meaningful signal.",analyticsvidhya,,
"You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?","Chances are, you might be tempted to say No, but that would be incorrect. Discarding correlated variables have a substantial effect on PCA because, in presence of correlated variables, the variance explained by a particular component gets inflated.

For example: You have 3 variables in a data set, of which 2 are correlated. If you run PCA on this data set, the first principal component would exhibit twice the variance than it would exhibit with uncorrelated variables. Also, adding correlated variables lets PCA put more importance on those variable, which is misleading.

 ",analyticsvidhya,,
"After spending several hours, you are now anxious to build a high accuracy model. As a result, you build 5 GBM models, thinking a boosting algorithm would do the magic. Unfortunately, neither of models could perform better than benchmark score. Finally, you decided to combine those models. Though, ensembled models are known to return high accuracy, but you are unfortunate. Where did you miss?","As we know, ensemble learners are based on the idea of combining weak learners to create strong learners. But, these learners provide superior result when the combined models are uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement, suggests that the models are correlated. The problem with correlated models is, all the models provide same information.

For example: If model 1 has classified User1122 as 1, there are high chances model 2 and model 3 would have done the same, even if its actual value is 0. Therefore, ensemble learners are built on the premise of combining weak uncorrelated models to obtain better predictions.",analyticsvidhya,,
 How is kNN different from kmeans clustering?,"Don’t get mislead by ‘k’ in their names. You should know that the fundamental difference between both these algorithms is, kmeans is unsupervised in nature and kNN is supervised in nature. kmeans is a clustering algorithm. kNN is a classification (or regression) algorithm.

kmeans algorithm partitions a data set into clusters such that a cluster formed is homogeneous and the points in each cluster are close to each other. The algorithm tries to maintain enough separability between these clusters. Due to unsupervised nature, the clusters have no labels.

kNN algorithm tries to classify an unlabeled observation based on its k (can be any number ) surrounding neighbors. It is also known as lazy learner because it involves minimal training of model. Hence, it doesn’t use training data to make generalization on unseen data set.",analyticsvidhya,,
How is True Positive Rate and Recall related? Write the equation.,"True Positive Rate = Recall. Yes, they are equal having the formula (TP/TP + FN).",analyticsvidhya,,
"You have built a multiple regression model. Your model R² isn’t as good as you wanted. For improvement, your remove the intercept term, your model R² becomes 0.8 from 0.3. Is it possible? How?","Yes, it is possible. We need to understand the significance of intercept term in a regression model. The intercept term shows model prediction without any independent variable i.e. mean prediction. The formula of R² = 1 – ∑(y – y´)²/∑(y – ymean)² where y´ is predicted value.   

When intercept term is present, R² value evaluates your model wrt. to the mean model. In absence of intercept term (ymean), the model can make no such evaluation, with large denominator, ∑(y - y´)²/∑(y)² equation’s value becomes smaller than actual, resulting in higher R².",analyticsvidhya,,
"After analyzing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if he’s true? Without losing any information, can you still build a better model?","To check multicollinearity, we can create a correlation matrix to identify & remove variables having correlation above 75% (deciding a threshold is subjective). In addition, we can use calculate VIF (variance inflation factor) to check the presence of multicollinearity. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Also, we can use tolerance as an indicator of multicollinearity.

But, removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalized regression models like ridge or lasso regression. Also, we can add some random noise in correlated variable so that the variables become different from each other. But, adding noise might affect the prediction accuracy, hence this approach should be carefully used.",analyticsvidhya,,
When is Ridge regression favorable over Lasso regression?,"You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in presence of few variables with medium / large sized effect, use lasso regression. In presence of many variables with small / medium sized effect, use ridge regression.

Conceptually, we can say, lasso regression (L1) does both variable selection and parameter shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all the coefficients in the model. In presence of correlated variables, ridge regression might be the preferred choice. Also, ridge regression works best in situations where the least square estimates have higher variance. Therefore, it depends on our model objective.",analyticsvidhya,,
Rise in global average temperature led to decrease in number of pirates around the world. Does that mean that decrease in number of pirates caused the climate change?,"After reading this question, you should have understood that this is a classic case of “causation and correlation”. No, we can’t conclude that decrease in number of pirates caused the climate change because there might be other factors (lurking or confounding variables) influencing this phenomenon.

Therefore, there might be a correlation between global average temperature and number of pirates, but based on this information we can’t say that pirated died because of rise in global average temperature.",analyticsvidhya,,
"While working on a data set, how do you select important variables? Explain your methods.","Following are the methods of variable selection you can use:

Remove the correlated variables prior to selecting important variables
Use linear regression and select variables based on p values
Use Forward Selection, Backward Selection, Stepwise Selection
Use Random Forest, Xgboost and plot variable importance chart
Use Lasso Regression
Measure information gain for the available set of features and select top n features accordingly.",analyticsvidhya,,
What is the difference between covariance and correlation?,"Correlation is the standardized form of covariance.

Covariances are difficult to compare. For example: if we calculate the covariances of salary ($) and age (years), we’ll get different covariances which can’t be compared because of having unequal scales. To combat such situation, we calculate correlation to get a value between -1 and 1, irrespective of their respective scale.",analyticsvidhya,,
"Is it possible capture the correlation between continuous and categorical variable? If yes, how?","Answer: Yes, we can use ANCOVA (analysis of covariance) technique to capture association between continuous and categorical variables.",analyticsvidhya,,
"Both being tree based algorithm, how is random forest different from Gradient boosting algorithm (GBM)?"," The fundamental difference is, random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions.

In bagging technique, a data set is divided into n samples using randomized sampling. Then, using a single learning algorithm a model is build on all samples. Later, the resultant predictions are combined using voting or averaging. Bagging is done is parallel. In boosting, after the first round of predictions, the algorithm weighs misclassified predictions higher, such that they can be corrected in the succeeding round. This sequential process of giving higher weights to misclassified predictions continue until a stopping criterion is reached.

Random forest improves model accuracy by reducing variance (mainly). The trees grown are uncorrelated to maximize the decrease in variance. On the other hand, GBM improves accuracy my reducing both bias and variance in a model.",analyticsvidhya,,
Running a binary classification tree algorithm is the easy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?,"A classification trees makes decision based on Gini Index and Node Entropy. In simple words, the tree algorithm find the best possible feature which can divide the data set into purest possible children nodes.

Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure. We can calculate Gini as following:

Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).
Calculate Gini for split using weighted Gini score of each node of that split
Entropy is the measure of impurity as given by (for binary class):

Entropy, Decision Tree

Here p and q is probability of success and failure respectively in that node. Entropy is zero when a node is homogeneous. It is maximum when a both the classes are present in a node at 50% – 50%.  Lower entropy is desirable.",analyticsvidhya,,
"You’ve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Haven’t you trained your model perfectly?","The model has overfitted. Training error 0.00 means the classifier has mimiced the training data patterns to an extent, that they are not available in the unseen data. Hence, when this classifier was run on unseen sample, it couldn’t find those patterns and returned prediction with higher error. In random forest, it happens when we use larger number of trees than necessary. Hence, to avoid these situation, we should tune number of trees using cross validation.",analyticsvidhya,,
You’ve got a data set to work having p (no. of variable) > n (no. of observation). Why is OLS as bad option to work with? Which techniques would be best to use? Why?,"In such high dimensional data sets, we can’t use classical regression techniques, since their assumptions tend to fail. When p > n, we can no longer calculate a unique least square coefficient estimate, the variances become infinite, so OLS cannot be used at all.

To combat this situation, we can use penalized regression methods like lasso, LARS, ridge which can shrink the coefficients to reduce variance. Precisely, ridge regression works best in situations where the least square estimates have higher variance.

Among other methods include subset regression, forward stepwise regression.

 ",analyticsvidhya,,
What is convex hull ? (Hint: Think SVM),"In case of linearly separable data, convex hull represents the outer boundaries of the two group of data points. Once convex hull is created, we get maximum margin hyperplane (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to create greatest separation between two groups.",analyticsvidhya,,
"We know that one hot encoding increasing the dimensionality of a data set. But, label encoding doesn’t. How ?","Don’t get baffled at this question. It’s a simple question asking the difference between the two.

Using one hot encoding, the dimensionality (a.k.a features) in a data set get increased because it creates a new variable for each level present in categorical variables. For example: let’s say we have a variable ‘color’. The variable has 3 levels namely Red, Blue and Green. One hot encoding ‘color’ variable will generate three new variables as Color.Red, Color.Blue and Color.Green containing 0 and 1 value.

In label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new variable is created. Label encoding is majorly used for binary variables.",analyticsvidhya,,
What cross validation technique would you use on time series data set? Is it k-fold or LOOCV?,"Neither.

In time series problem, k fold can be troublesome because there might be some pattern in year 4 or 5 which is not in year 3. Resampling the data set will separate these trends, and we might end up validation on past years, which is incorrect. Instead, we can use forward chaining strategy with 5 fold as shown below:

fold 1 : training [1], test [2]
fold 2 : training [1 2], test [3]
fold 3 : training [1 2 3], test [4]
fold 4 : training [1 2 3 4], test [5]
fold 5 : training [1 2 3 4 5], test [6]
where 1,2,3,4,5,6 represents “year”.",analyticsvidhya,,
"You are given a data set consisting of variables having more than 30% missing values? Let’s say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?","We can deal with them in the following ways:

Assign a unique category to missing values, who knows the missing values might decipher some trend
We can remove them blatantly.
Or, we can sensibly check their distribution with the target variable, and if found any pattern we’ll keep those missing values and assign them a new category while removing others.",analyticsvidhya,,
"‘People who bought this, also bought…’ recommendations seen on amazon is a result of which algorithm?","The basic idea for this kind of recommendation engine comes from collaborative filtering.

Collaborative Filtering algorithm considers “User Behavior” for recommending items. They exploit behavior of other users and items in terms of transaction history, ratings, selection and purchase information. Other users behaviour and preferences over the items are used to recommend items to the new users. In this case, features of the items are not known.",analyticsvidhya,,
What do you understand by Type I vs Type II error ?,"Type I error is committed when the null hypothesis is true and we reject it, also known as a ‘False Positive’. Type II error is committed when the null hypothesis is false and we accept it, also known as ‘False Negative’.

In the context of confusion matrix, we can say Type I error occurs when we classify a value as positive (1) when it is actually negative (0). Type II error occurs when we classify a value as negative (0) when it is actually positive(1).",analyticsvidhya,,
"You are working on a classification problem. For validation purposes, you’ve randomly sampled the training data set into train and validation. You are confident that your model will work incredibly well on unseen data since your validation accuracy is high. However, you get shocked after getting poor test accuracy. What went wrong?","In case of classification problem, we should always use stratified sampling instead of random sampling. A random sampling doesn’t takes into consideration the proportion of target classes. On the contrary, stratified sampling helps to maintain the distribution of target variable in the resultant distributed samples also.

 ",analyticsvidhya,,
"You have been asked to evaluate a regression model based on R², adjusted R² and tolerance. What will be your criteria?","Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of percent of variance in a predictor which cannot be accounted by other predictors. Large values of tolerance is desirable.

We will consider adjusted R² as opposed to R² to evaluate model fit because R² increases irrespective of improvement in prediction accuracy as we add more variables. But, adjusted R² would only increase if an additional variable improves the accuracy of model, otherwise stays same. It is difficult to commit a general threshold value for adjusted R² because it varies between data sets. For example: a gene mutation data set might result in lower adjusted R² and still provide fairly good predictions, as compared to a stock market data where lower adjusted R² implies that model is not good.",analyticsvidhya,,
"In k-means or kNN, we use euclidean distance to calculate the distance between nearest neighbors. Why not manhattan distance ?","We don’t use manhattan distance because it calculates distance horizontally or vertically only. It has dimension restrictions. On the other hand, euclidean metric can be used in any space to calculate distance. Since, the data points can be present in any dimension, euclidean distance is a more viable option.

Example: Think of a chess board, the movement made by a bishop or a rook is calculated by manhattan distance because of their respective vertical & horizontal movements.",analyticsvidhya,,
Explain machine learning to me like a 5 year old.,"It’s simple. It’s just like how babies learn to walk. Every time they fall down, they learn (unconsciously) & realize that their legs should be straight and not in a bend position. The next time they fall down, they feel pain. They cry. But, they learn ‘not to stand like that again’. In order to avoid that pain, they try harder. To succeed, they even seek support from the door or wall or anything near them, which helps them stand firm.

This is how a machine works & develops intuition from its environment.",analyticsvidhya,,
 I know that a linear regression model is generally evaluated using Adjusted R² or F value. How would you evaluate a logistic regression model?,"We can use the following methods:

Since logistic regression is used to predict probabilities, we can use AUC-ROC curve along with confusion matrix to determine its performance.
Also, the analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.
Null Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model. Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model.",analyticsvidhya,,
"Considering the long list of machine learning algorithm, given a data set, how do you decide which one to use?","You should say, the choice of machine learning algorithm solely depends of the type of data. If you are given a data set which is exhibits linearity, then linear regression would be the best algorithm to use. If you given to work on images, audios, then neural network would help you to build a robust model.

If the data comprises of non linear interactions, then a boosting or bagging algorithm should be the choice. If the business requirement is to build a model which can be deployed, then we’ll use regression or a decision tree model (easy to interpret and explain) instead of black box algorithms like SVM, GBM etc.

In short, there is no one master algorithm for all situations. We must be scrupulous enough to understand which algorithm to use.",analyticsvidhya,,
Do you suggest that treating a categorical variable as continuous variable would result in a better predictive model?,"For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature.",analyticsvidhya,,
When does regularization becomes necessary in Machine Learning?,"Regularization becomes necessary when the model begins to ovefit / underfit. This technique introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term. This helps to reduce model complexity so that the model can become better at predicting (generalizing).",analyticsvidhya,,
What do you understand by Bias Variance trade off?,"The error emerging from any model can be broken down into three components mathematically. Following are these component :

error of a model

Bias error is useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have a under-performing model which keeps on missing important trends. Variance on the other side quantifies how are the prediction made on same observation different from each other. A high variance model will over-fit on your training population and perform badly on any observation beyond training.

 ",analyticsvidhya,,
OLS is to linear regression. Maximum likelihood is to logistic regression. Explain the statement.,"OLS and Maximum likelihood are the methods used by the respective regression methods to approximate the unknown parameter (coefficient) value. In simple words,

Ordinary least square(OLS) is a method used in linear regression which approximates the parameters resulting in minimum distance between actual and predicted values. Maximum Likelihood helps in choosing the the values of parameters which maximizes the likelihood that the parameters are most likely to produce observed data.",analyticsvidhya,,
Explain what regularization is and why it is useful.,"Regularization is the process of adding a tuning parameter to a model to induce smoothness in order to prevent overfitting. (see also KDnuggets posts on Overfitting) 


This is most often done by adding a constant multiple to an existing weight vector. This constant is often either the L1 (Lasso) or L2 (ridge), but can in actuality can be any norm. The model predictions should then minimize the mean of the loss function calculated on the regularized training set.",kdnuggets,,
Which data scientists do you admire most? which startups?,"This question does not have a correct answer, but here is my personal list of 12 Data Scientists I most admire, not in any particular order. .  
Geoff Hinton, Yann LeCun, and Yoshua Bengio - for persevering with Neural Nets when and starting the current Deep Learning revolution. 

Demis Hassabis, for his amazing work on DeepMind, which achieved human or superhuman performance on Atari games and recently Go. 

Jake Porway from DataKind and Rayid Ghani from U. Chicago/DSSG, for enabling data science contributions to social good. 

DJ Patil, First US Chief Data Scientist, for using Data Science to make US government work better. 

Kirk D. Borne for his influence and leadership on social media. 

Claudia Perlich for brilliant work on ad ecosystem and serving as a great KDD-2014 chair. 

Hilary Mason for great work at Bitly and inspiring others as a Big Data Rock Star. Usama Fayyad, for showing leadership and setting high goals for KDD and Data Science, which helped inspire me and many thousands of others to do their best. 

Hadley Wickham, for his fantastic work on Data Science and Data Visualization in R, including dplyr, ggplot2, and Rstudio. 

There are too many excellent startups in Data Science area, but I will not list them here to avoid a conflict of interest. 

Here is some of our previous coverage of startups. ",kdnuggets,,
How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression.,"Proposed methods for model validation: 

If the values predicted by the model are far outside of the response variable range, this would immediately indicate poor estimation or model inaccuracy.
If the values seem to be reasonable, examine the parameters; any of the following would indicate poor estimation or multi-collinearity: opposite signs of expectations, unusually large or small values, or observed inconsistency when the model is fed new data.
Use the model for prediction by feeding it new data, and use the coefficient of determination (R squared) as a model validity measure.
Use data splitting to form a separate dataset for estimating model parameters, and another for validating predictions.
Use jackknife resampling if the dataset contains a small number of instances, and measure validity with R squared and mean squared error (MSE).",kdnuggets,,
Explain what precision and recall are. How do they relate to the ROC curve?,"Calculating precision and recall is actually quite easy. Imagine there are 100 positive cases among 10,000 cases. You want to predict which ones are positive, and you pick 200 to have a better chance of catching many of the 100 positive cases.  You record the IDs of your predictions, and when you get the actual results you sum up how many times you were right or wrong. There are four ways of being right or wrong:

TN / True Negative: case was negative and predicted negative
TP / True Positive: case was positive and predicted positive
FN / False Negative: case was positive but predicted negative
FP / False Positive: case was negative but predicted positive
Makes sense so far? Now you count how many of the 10,000 cases fall in each bucket, say:",kdnuggets,,
How can you prove that one improvement you've brought to an algorithm is really an improvement over not doing anything? Are you familiar with A/B testing?,"Often it is observed that in the pursuit of rapid innovation (aka ""quick fame""), the principles of scientific methodology are violated leading to misleading innovations, i.e. appealing insights that are confirmed without rigorous validation. One such scenario is the case that given the task of improving an algorithm to yield better results, you might come with several ideas with potential for improvement. 

An obvious human urge is to announce these ideas ASAP and ask for their implementation. When asked for supporting data, often limited results are shared, which are very likely to be impacted by selection bias (known or unknown) or a misleading global minima (due to lack of appropriate variety in test data). 

Data scientists do not let their human emotions overrun their logical reasoning. While the exact approach to prove that one improvement you've brought to an algorithm is really an improvement over not doing anything would depend on the actual case at hand, there are a few common guidelines:
Ensure that there is no selection bias in test data used for performance comparison
Ensure that the test data has sufficient variety in order to be symbolic of real-life data (helps avoid overfitting)
Ensure that ""controlled experiment"" principles are followed i.e. while comparing performance, the test environment (hardware, etc.) must be exactly the same while running original algorithm and new algorithm
Ensure that the results are repeatable with near similar results
Examine whether the results reflect local maxima/minima or global maxima/minima

 
One common way to achieve the above guidelines is through A/B testing, where both the versions of algorithm are kept running on similar environment for a considerably long time and real-life input data is randomly split between the two. This approach is particularly common in Web Analytics. ",kdnuggets,,
What is root cause analysis? How to identify a cause vs. a correlation? Give examples.,"According to Wikipedia, 
Root cause analysis (RCA) is a method of problem solving used for identifying the root causes of faults or problems. A factor is considered a root cause if removal thereof from the problem-fault-sequence prevents the final undesirable event from recurring; whereas a causal factor is one that affects an event's outcome, but is not a root cause.


Root cause analysis was initially developed to analyze industrial accidents, but is now widely used in other areas, such as healthcare, project management, or software testing. 

Here is a useful Root Cause Analysis Toolkit from the state of Minnesota. 

Essentially, you can find the root cause of a problem and show the relationship of causes by repeatedly asking the question, ""Why?"", until you find the root of the problem. This technique is commonly called ""5 Whys"", although is can be involve more or less than 5 questions. ",kdnuggets,,
"Are you familiar with price optimization, price elasticity, inventory management, competitive intelligence? Give examples.","Those are economics terms that are not frequently asked of Data Scientists but they are useful to know. 

Price optimization is the use of mathematical tools to determine how customers will respond to different prices for its products and services through different channels. 

Big Data and data mining enables use of personalization for price optimization. Now companies like Amazon can even take optimization further and show different prices to different visitors, based on their history, although there is a strong debate about whether this is fair. 

Price elasticity in common usage typically refers to
Price elasticity of demand, a measure of price sensitivity. It is computed as: 
Price Elasticity of Demand = % Change in Quantity Demanded / % Change in Price.

 
Similarly, Price elasticity of supply is an economics measure that shows how the quantity supplied of a good or service responds to a change in its price. 

Inventory management is the overseeing and controlling of the ordering, storage and use of components that a company will use in the production of the items it will sell as well as the overseeing and controlling of quantities of finished products for sale. 

Wikipedia defines 
Competitive intelligence: the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers making strategic decisions for an organization.


Tools like Google Trends, Alexa, Compete, can be used to determine general trends and analyze your competitors on the web. ",kdnuggets,,
What is statistical power?,"Wikipedia defines Statistical power or sensitivity of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is true. 

To put in another way, Statistical power is the likelihood that a study will detect an effect when the effect is present. The higher the statistical power, the less likely you are to make a Type II error (concluding there is no effect when, in fact, there is). 

Here are some tools to calculate statistical power. ",kdnuggets,,
Explain what resampling methods are and why they are useful. Also explain their limitations.,"Classical statistical parametric tests compare observed statistics to theoretical sampling distributions. Resampling a data-driven, not theory-driven methodology which is based upon repeated sampling within the same sample. 

Resampling refers to methods for doing one of these
Estimating the precision of sample statistics (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping)
Exchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests)
Validating models by using random subsets (bootstrapping, cross validation)",kdnuggets,,
"Is it better to have too many false positives, or too many false negatives? Explain","It depends on the question as well as on the domain for which we are trying to solve the question. 

In medical testing, false negatives may provide a falsely reassuring message to patients and physicians that disease is absent, when it is actually present. This sometimes leads to inappropriate or inadequate treatment of both the patient and their disease. So, it is desired to have too many false positive. 

For spam filtering, a false positive occurs when spam filtering or spam blocking techniques wrongly classify a legitimate email message as spam and, as a result, interferes with its delivery. While most anti-spam tactics can block or filter a high percentage of unwanted emails, doing so without creating significant false-positive results is a much more demanding task. So, we prefer too many false negatives over many false positives. ",kdnuggets,,
"What is selection bias, why is it important and how can you avoid it?","Selection bias, in general, is a problematic situation in which error is introduced due to a non-random population sample. For example, if a given sample of 100 test cases was made up of a 60/20/15/5 split of 4 classes which actually occurred in relatively equal numbers in the population, then a given model may make the false assumption that probability could be the determining predictive factor. Avoiding non-random samples is the best way to deal with bias; however, when this is impractical, techniques such as resampling, boosting, and weighting are strategies which can be introduced to help deal with the situation. ",kdnuggets,,
What is the difference between supervised and unsupervised machine learning?,"Supervised Machine learning:
Supervised machine learning requires training labeled data.

Unsupervised Machine learning:
Unsupervised machine learning doesn’t required labeled data.",nitin-panwar,,
"What is bias, variance trade off ?","Bias:
“Bias is error introduced in your model due to over simplification of machine learning algorithm.” It can lead to underfitting. When you train your model at that time model makes simplified assumptions to make the target function easier to understand.

Low bias machine learning algorithms - Decision Trees, k-NN and SVM
Hight bias machine learning algorithms - Liear Regression, Logistic Regression
Variance:
“Variance is error introduced in your model due to complex machine learning algorithm, your model learns noise also from the training dataset and performs bad on test dataset.” It can lead high sensitivity and overfitting.

Normally, as you increase the complexity of your model, you will see a reduction in error due to lower bias in the model. However, this only happens till a particular point. As you continue to make your model more complex, you end up over-fitting your model and hence your model will start suffering from high variance.

Bias variance trade off

Bias, Variance trade off:
The goal of any supervised machine learning algorithm is to have low bias and low variance to achive good prediction performance.

The k-nearest neighbors algorithm has low bias and high variance, but the trade-off can be changed by increasing the value of k which increases the number of neighbors that contribute to the prediction and in turn increases the bias of the model.

The support vector machine algorithm has low bias and high variance, but the trade-off can be changed by increasing the C parameter that influences the number of violations of the margin allowed in the training data which increases the bias but decreases the variance.

There is no escaping the relationship between bias and variance in machine learning.

Increasing the bias will decrease the variance. Increasing the variance will decrease the bias.",nitin-panwar,,
3. What is exploding gradients ?,"“Exploding gradients are a problem where large error gradients accumulate and result in very large updates to neural network model weights during training.” At an extreme, the values of weights can become so large as to overflow and result in NaN values.

This has the effect of your model being unstable and unable to learn from your training data. Now let’s understand what is the gradient.

Gradient:
Gradient is the direction and magnitude calculated during training of a neural network that is used to update the network weights in the right direction and by the right amount.",nitin-panwar,,
What is a confusion matrix ?,"The confusion matrix is a 2X2 table that contains 4 outputs provided by the binary classifier. Various measures, such as error-rate, accuracy, specificity, sensitivity, precision and recall are derived from it. Confusion Matrix.  A dataset used for performance evaluation is called test dataset. It should contain the correct labels and predicted labels.  The predicted labels will exactly the same if the performance of a binary classfier is perfect.",nitin-panwar,,
Explain how a ROC curve works ?,The ROC curve is a graphical representation of the contrast between true positive rates and false positive rates at various thresholds. It is often used as a proxy for the trade-off between the sensitivity(true positive rate) and false positive rate.,nitin-panwar,,
What is selection Bias ?,Selection bias occurs when sample obtained is not represantative of the population intended to be analyzed.,nitin-panwar,,
Explain SVM machine learning algorithm in detail.,"SVM stands for support vector machine, it is a supervised machine learning algorithm which can be used for both Regression and Classification. If you have n features in your training dataset, SVM tries to plot it in n-dimentional space with the value of each feature being the value of a particular coordinate. SVM uses hyper planes to seperate out different classes based on the provided kernel function.",nitin-panwar,,
What are support vectors in SVM.,In the above diagram we see that the thinner lines mark the distance from the classifier to the closest data points called the support vectors (darkened data points). The distance between the two thin lines is called the margin.,nitin-panwar,,
What are the different kernels functions in SVM ?,"There are four types of kernels in SVM.

Linear Kernel
Polynomial kernel
Radial basis kernel
Sigmoid kernel",nitin-panwar,,
Explain Decision Tree algorithm in detail.,Decision tree is a supervised machine learning algorithm mainly used for the Regression and Classification.It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. Decision tree can handle both categorical and numerical data.,nitin-panwar,,
What is Entropy and Information gain in Decision tree algorithm ?,"The core algorithm for building decision tree is called ID3. ID3 uses Enteropy and Information Gain to construct a decision tree.

Entropy

A decision tree is built top-down from a root node and involve partitioning of data into homogenious subsets. ID3 uses enteropy to check the homogeneity of a sample. If the sample is completely homogenious then entropy is zero and if the sample is an equally divided it has entropy of one.
Information Gain

The Information Gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attributes that returns the highest information gain.",nitin-panwar,,
What is pruning in Decision Tree ?,"When we remove sub-nodes of a decision node, this procsss is called pruning or opposite process of splitting.",nitin-panwar,,
What is Ensemble Learning ?,"Ensemble is the art of combining diverse set of learners(Individual models) togather to improvise on the stability and predictive power of the model. Ensemble learning has many types but two more popular ensemble learning techniques are mentioned below.

Bagging

Bagging tries to implement similar learners on small sample populations and then takes a mean of all the predictions. In generalized bagging, you can use different learners on different population. As you expect this helps us to reduce the variance error.

Boosting

Boosting is an iterative technique which adjust the weight of an observation based on the last classification. If an observation was classfied incorrectly, it tries to increase the weight of this observation and vice versa. Boosting in general decreases the bias error and builds strong predictive models. However, they may overfit on the training data.

",nitin-panwar,,
What is Random Forest? How does it work ?,"Random forest is a versatile machine learning method capable of performing both regression and classification tasks. It is also used for dimentionality reduction, treats missing values, outlier values. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model.

In Random Forest, we grow multiple trees as opposed to a single tree. To classify a new object based on attributes, each tree gives a classification. The forest chooses the classification having the most votes(Over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.",nitin-panwar,,
What cross-validation technique would you use on a time series dataset.,"Instead of using k-fold cross-validation, you should be aware to the fact that a time series is not randomly distributed data - It is inherently ordered by chronological order.

In case of time series data, you should use techniques like forward chaining – Where you will be model on past data then look at forward-facing data.

fold 1: training[1], test[2]

fold 1: training[1 2], test[3]

fold 1: training[1 2 3], test[4]

fold 1: training[1 2 3 4], test[5]",nitin-panwar,,
What is logistic regression? Or State an example when you have used logistic regression recently.,"Logistic Regression often referred as logit model is a technique to predict the binary outcome from a linear combination of predictor variables. For example, if you want to predict whether a particular political leader will win the election or not. In this case, the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). The predictor variables here would be the amount of money spent for election campaigning of a particular candidate, the amount of time spent in campaigning, etc.",nitin-panwar,,
What do you understand by the term Normal Distribution?,"Data is usually distributed in different ways with a bias to the left or to the right or it can all be jumbled up. However, there are chances that data is distributed around a central value without any bias to the left or right and reaches normal distribution in the form of a bell shaped curve. The random variables are distributed in the form of an symmetrical bell shaped curve.",nitin-panwar,,
What is a Box Cox Transformation?,"Dependent variable for a regression analysis might not satisfy one or more assumptions of an ordinary least squares regression. The residuals could either curve as the prediction increases or follow skewed distribution. In such scenarios, it is necessary to transform the response variable so that the data meets the required assumptions. A Box cox transformation is a statistical technique to transform non-normal dependent variables into a normal shape. If the given data is not normal then most of the statistical techniques assume normality. Applying a box cox transformation means that you can run a broader number of tests.

A Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques, if your data isn’t normal, applying a Box-Cox means that you are able to run a broader number of tests. The Box Cox transformation is named after statisticians George Box and Sir David Roxbee Cox who collaborated on a 1964 paper and developed the technique.",nitin-panwar,,
How will you define the number of clusters in a clustering algorithm?,"Though the Clustering Algorithm is not specified, this question will mostly be asked in reference to K-Means clustering where “K” defines the number of clusters. For example, the following image shows three different groups.   Within Sum of squares is generally used to explain the homogeneity within a cluster. If you plot WSS for a range of number of clusters, you will get the plot shown below. The Graph is generally known as Elbow Curve.  graph i.e. Number of Cluster =6 is the point after which you don’t see any decrement in WSS. This point is known as bending point and taken as K in K – Means.This is the widely used approach but few data scientists also use Hierarchical clustering first to create dendograms and identify the distinct groups from there.",nitin-panwar,,
What is deep learning?,"Deep learning is subfield of machine learning inspired by structure and function of brain called artificial neural network. We have a lot numbers of algorithms under machine learning like Linear regression, SVM, Neural network etc and deep learning is just an extention of Neural networks. In neural nets we consider small number of hidden layers but when it comes to deep learning algorithms we consider a huge number of hidden latyers to better understand the input output relationship.",nitin-panwar,,
What are Recurrent Neural Networks(RNNs) ?,"Recurrent nets are type of artifical neural networks designed to recognize pattern from the sequence of data such as Time series, stock market and goverment agencis etc. To understand recurrent nets, first you have to understand the basics of feedforward nets. Both these networks RNN and feedforward named after the way they channel information throgh a series of mathematical oprations performed at the nodes of the network. One feeds information throgh straight(never touching same node twice), while the other cycles it throgh loop, and the latter are called recurrent.  Recurrent networks on the other hand, take as their input not just the current input example they see, but also the what they have percieved previously in time. The BTSXPE at the bottom of the drawing represents the input example in the current moment, and CONTEXT UNIT represents the output of the previous moment. The decision a recurrent neural network reached at time t-1 affects the decision that it will reach one moment later at time t. So recurrent networks have two sources of input, the present and the recent past, which combine to determine how they respond to new data, much as we do in life.

The error they generate will return via backpropagation and be used to adjust their weights until error can’t go any lower. Remember, the purpose of recurrent nets is to accurately classify sequential input. We rely on the backpropagation of error and gradient descent to do so.

Backpropagation in feedforward networks moves backward from the final error through the outputs, weights and inputs of each hidden layer, assigning those weights responsibility for a portion of the error by calculating their partial derivatives – ∂E/∂w, or the relationship between their rates of change. Those derivatives are then used by our learning rule, gradient descent, to adjust the weights up or down, whichever direction decreases error.

Recurrent networks rely on an extension of backpropagation called backpropagation through time, or BPTT. Time, in this case, is simply expressed by a well-defined, ordered series of calculations linking one time step to the next, which is all backpropagation needs to work.",nitin-panwar,,
What is the difference between machine learning and deep learning?,"Machine learning:
Machine learning is a field of computer science that gives computers the ability to learn without being explicitly programmed. Machine learning can be categorized in following three categories.

Supervised machine learning,
Unsupervised machine learning,
Reinforcement learning
Deep learning:
Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.",nitin-panwar,,
What is reinforcement learning ?,"Reinforcement Learning is learning what to do and how to map situations to actions. The end result is to maximize the numerical reward signal. The learner is not told which action to take, but instead must discover which action will yield the maximum reward.Reinforcement learning is inspired by the learning of human beings, it is based on the reward/panelity mechanism.",nitin-panwar,,
What is selection bias ?,"Selection Bias

Selection bias is the bias introduced by the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the selection effect. The phrase “selection bias” most often refers to the distortion of a statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may not be accurate",nitin-panwar,,
Explain what regularization is and why it is useful.,"Regularization

Regularization is the process of adding tunning parameter to a model to induce smoothness in order to prevent overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constant is often the L1(Lasso) or L2(ridge). The model predictions should then minimize the loss function calculated on the regularized training set.",nitin-panwar,,
What is TF/IDF vectorization ?,"tf–idf is short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.",nitin-panwar,,
What are Recommender Systems?,"A subclass of information filtering systems that are meant to predict the preferences or ratings that a user would give to a product. Recommender systems are widely used in movies, news, research articles, products, social tags, music, etc.",nitin-panwar,,
What is the difference between Regression and classification ML techniques.,"Both Regression and classification machine learning techniques come under Supervised machine learning algorithms. In Supervised machine learning algorithm, we have to train the model using labeled dataset, While training we have to explicitly provide the correct labels and algorithm tries to learn the pattern from input to output. If our labels are discreate values then it will a classification problem, e.g A,B etc. but if our labels are continuous values then it will be a regression problem, e.g 1.23, 1.333 etc.",nitin-panwar,,
If you are having 4GB RAM in your machine and you want to train your model on 10GB dataset. How would you go about this problem. Have you ever faced this kind of problem in your machine learning/data science experience so far ?,"First of all you have to ask which ML model you want to train.

For Neural networks: Batch size with Numpy array will work.

Steps:

Load the whole data in Numpy array. Numpy array has property to create mapping of complete dataset, it doesn’t load complete dataset in memory.
You can pass index to Numpy array to get required data.
Use this data to pass to Neural network.
Have small batch size.
For SVM: Partial fit will work

Steps:

Divide one big dataset in small size datasets.
Use partialfit method of SVM, it requires subset of complete dataset.
Repeat step 2 for other subsets.",nitin-panwar,,
 What is p-value?,"When you perform a hypothesis test in statistics, a p-value can help you determine the strength of your results. p-value is a number between 0 and 1. Based on the value it will denote the strength of the results. The claim which is on trial is called Null Hypothesis.

Low p-value (≤ 0.05) indicates strength against the null hypothesis which means we can reject the null Hypothesis. High p-value (≥ 0.05) indicates strength for the null hypothesis which means we can accept the null Hypothesis p-value of 0.05 indicates the Hypothesis could go either way. To put it in another way,

High P values: your data are likely with a true null. Low P values: your data are unlikely with a true null.

32. What is ‘N",nitin-panwar,,
What is ‘Naive’ in a Naive Bayes ?,"The Naive Bayes Algorithm is based on the Bayes Theoram. Bayes’ theoram describes the probablitiy of an event, based on prior knowledge of conditions that might be related to the event.

",nitin-panwar,,
"What is Data Science? Also, list the differences between supervised and unsupervised learning.","Data Science involves using automated methods to analyze massive amounts of data and to extract knowledge from them. By combining aspects of statistics, computer science, applied mathematics, and visualization, data science can turn the vast amounts of data the digital age generates into new insights and new knowledge.",edureka,,
What are the important skills to have in Python with regard to data analysis?,"The following are some of the important skills to possess which will come handy when performing data analysis using Python.

Good understanding of the built-in data types especially lists, dictionaries, tuples and sets.
Mastery of N-dimensional NumPy arrays.
Mastery of pandas dataframes.
Ability to perform element-wise vector and matrix operations on NumPy arrays. This requires the biggest shift in mindset for someone coming from a traditional software development background who’s used to for loops.
Knowing that you should use the Anaconda distribution and the conda package manager.
Familiarity with scikit-learn.
Ability to write efficient list comprehensions instead of traditional for loops.
Ability to write small, clean functions (important for any developer), preferably pure functions that don’t alter objects.
Knowing how to profile the performance of a Python script and how to optimize bottlenecks.
The following will help to tackle any problem in data analytics and machine learning.",edureka,,
What is Selection Bias?,"Selection bias is the bias introduced by the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the selection effect. It is the distortion of a statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may not be accurate.

The types of selection bias includes:

Sampling bias: It is a systematic error due to a non-random sample of a population causing some members of the population to be less likely to be included than others resulting in a biased sample.
Time interval: A trial may be terminated early at an extreme value (often for ethical reasons), but the extreme value is likely to be reached by the variable with the largest variance, even if all variables have a similar mean.
Data: When specific subsets of data are chosen to support a conclusion or rejection of bad data on arbitrary grounds, instead of according to previously stated or generally agreed criteria.
Attrition: Attrition bias is a kind of selection bias caused by attrition (loss of participants) discounting trial subjects/tests that did not run to completion.",edureka,,
What is the difference between “long” and “wide” format data?,"In the wide format, a subject’s repeated responses will be in a single row, and each response is in a separate column.

In the long format, each row is a one time point per subject.",edureka,,
What do you understand by the term Normal Distribution?,"Data is usually distributed in different ways with a bias to the left or to the right or it can all be jumbled up.

However, there are chances that data is distributed around a central value without any bias to the left or right and reaches normal distribution in the form of a bell-shaped curve.",edureka,,
What is the goal of A/B Testing?,"It is a statistical hypothesis testing for randomized experiment with two variables A and B.

The goal of A/B Testing is to identify any changes to the web page to maximize or increase the outcome of an interest.

An example for this could be identifying the click-through rate for a banner ad.",edureka,,
What do you understand by statistical power of sensitivity and how do you calculate it?,"Sensitivity is commonly used to validate the accuracy of a classifier (Logistic, SVM, Random Forest etc.).

Sensitivity is nothing but “Predicted True events/ Total events”. True events here are the events which were true and model also predicted them as true.

Calculation of seasonality is pretty straight forward.

Seasonality = ( True Positives ) / ( Positives in Actual Dependent Variable )

where true positives are positive events which are correctly classified as positives.",edureka,,
What are the differences between overfitting and underfitting?,"In statistics and machine learning, one of the most common tasks is to fit a model to a set of training data, so as to be able to make reliable predictions on general untrained data.

In overfitting, a statistical model describes random error or noise instead of the underlying relationship. Overfitting occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. A model that has been overfit has poor predictive performance, as it overreacts to minor fluctuations in the training data.

Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. Underfitting would occur, for example, when fitting a linear model to non-linear data. Such a model too would have poor predictive performance.",edureka,,
Python or R – Which one would you prefer for text analytics?,"We will prefer Python because of the following reasons:

Python would be the best option because it has Pandas library that provides easy to use data structures and high-performance data analysis tools.
R is more suitable for machine learning than just text analysis.
Python performs faster for all types of text analytics.",edureka,,
How does data cleaning plays a vital role in analysis?,"Data cleaning can help in analysis because:

Cleaning data from multiple sources helps to transform it into a format that data analysts or data scientists can work with.
Data Cleaning helps to increase the accuracy of the model in machine learning.
It is a cumbersome process because as the number of data sources increases, the time taken to clean the data increases exponentially due to the number of sources and the volume of data generated by these sources.
It might take up to 80% of the time for just cleaning data making it a critical part of analysis task.",edureka,,
"Differentiate between univariate, bivariate and multivariate analysis.","Univariate analyses are descriptive statistical analysis techniques which can be differentiated based on the number of variables involved at a given point of time. For example, the pie charts of sales based on territory involve only one variable and can the analysis can be referred to as univariate analysis.

Bivariate analysis attempts to understand the difference between two variables at a time as in a scatterplot. For example, analyzing the volume of sale and a spending can be considered as an example of bivariate analysis.

Multivariate analysis deals with the study of more than two variables to understand the effect of variables on the responses.",edureka,,
What is Cluster Sampling?,Cluster sampling is a technique used when it becomes difficult to study the target population spread across a wide area and simple random sampling cannot be applied. Cluster Sample is a probability sample where each sampling unit is a collection or cluster of elements.,edureka,,
What is Systematic Sampling?,"Systematic sampling is a statistical technique where elements are selected from an ordered sampling frame. In systematic sampling, the list is progressed in a circular manner so once you reach the end of the list, it is progressed from the top again. The best example of systematic sampling is equal probability method.",edureka,,
What are Eigenvectors and Eigenvalues?,"Eigenvectors are used for understanding linear transformations. In data analysis, we usually calculate the eigenvectors for a correlation or covariance matrix. Eigenvectors are the directions along which a particular linear transformation acts by flipping, compressing or stretching.

Eigenvalue can be referred to as the strength of the transformation in the direction of eigenvector or the factor by which the compression occurs.",edureka,,
Can you cite some examples where a false positive is important than a false negative?,"Let us first understand what false positives and false negatives are. False positives are the cases where you wrongly classified a non-event as an event a.k.a Type I error. False negatives are the cases where you wrongly classify events as non-events, a.k.a Type II error.

Example 1: In the medical field, assume you have to give chemotherapy to patients. Assume a patient comes to that hospital and he is tested positive for cancer, based on the lab prediction but he actually doesn’t have cancer. This is a case of false positive. Here it is of utmost danger to start chemotherapy on this patient when he actually does not have cancer. In the absence of cancerous cell, chemotherapy will do certain damage to his normal healthy cells and might lead to severe diseases, even cancer.

Example 2: Let’s say an e-commerce company decided to give $1000 Gift voucher to the customers whom they assume to purchase at least $10,000 worth of items. They send free voucher mail directly to 100 customers without any minimum purchase condition because they assume to make at least 20% profit on sold items above $10,000. Now the issue is if we send the $1000 gift vouchers to customers who have not actually purchased anything but are marked as having made $10,000 worth of purchase.",edureka,,
Can you cite some examples where a false negative important than a false positive?,"Example 1: Assume there is an airport ‘A’ which has received high-security threats and based on certain characteristics they identify whether a particular passenger can be a threat or not. Due to a shortage of staff, they decide to scan passengers being predicted as risk positives by their predictive model. What will happen if a true threat customer is being flagged as non-threat by airport model?

Example 2: What if Jury or judge decide to make a criminal go free?

Example 3: What if you rejected to marry a very good person based on your predictive model and you happen to meet him/her after few years and realize that you had a false negative?",edureka,,
Can you cite some examples where both false positive and false negatives are equally important?,"In the banking industry giving loans is the primary source of making money but at the same time if your repayment rate is not good you will not make any profit, rather you will risk huge losses.

Banks don’t want to lose good customers and at the same point in time, they don’t want to acquire bad customers. In this scenario, both the false positives and false negatives become very important to measure.",edureka,,
Can you explain the difference between a Validation Set and a Test Set?,"Validation set can be considered as a part of the training set as it is used for parameter selection and to avoid overfitting of the model being built.

On the other hand, a test set is used for testing or evaluating the performance of a trained machine learning model.

In simple terms, the differences can be summarized as; training set is to fit the parameters i.e. weights and test set is to assess the performance of the model i.e. evaluating the predictive power and generalization.",edureka,,
Explain cross-validation.,"Cross validation is a model validation technique for evaluating how the outcomes of a statistical analysis will generalize to an independent data set. Mainly used in backgrounds where the objective is forecast and one wants to estimate how accurately a model will accomplish in practice.

The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data set) in order to limit problems like overfitting and get an insight on how the model will generalize to an independent data set.",edureka,,
What is Machine Learning?,Machine Learning explores the study and construction of algorithms that can learn from and make predictions on data. Closely related to computational statistics. Used to devise complex models and algorithms that lend themselves to a prediction which in commercial use is known as predictive analytics.,edureka,,
What is the Supervised Learning?,"Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples.

Algorithms: Support Vector Machines, Regression, Naive Bayes, Decision Trees, K-nearest Neighbor Algorithm and Neural Networks

E.g. If you built a fruit classifier, the labels will be “this is an orange, this is an apple and this is a banana”, based on showing the classifier examples of apples, oranges and bananas.",edureka,,
What is Unsupervised learning?,"Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses.

Algorithms: Clustering, Anomaly Detection, Neural Networks and Latent Variable Models

E.g. In the same example, a fruit clustering will categorize as “fruits with soft skin and lots of dimples”, “fruits with shiny hard skin” and “elongated yellow fruits”.",edureka,,
What are the various classification algorithms?,The below diagram lists the most important classification algorithms.  https://www.edureka.co/blog/interview-questions/data-science-interview-questions/,edureka,,
What is logistic regression? State an example when you have used logistic regression recently.,"Logistic Regression often referred as logit model is a technique to predict the binary outcome from a linear combination of predictor variables. 

For example, if you want to predict whether a particular political leader will win the election or not. In this case, the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). The predictor variables here would be the amount of money spent for election campaigning of a particular candidate, the amount of time spent in campaigning, etc.",edureka,,
What are Recommender Systems?,"Recommender Systems are a subclass of information filtering systems that are meant to predict the preferences or ratings that a user would give to a product. Recommender systems are widely used in movies, news, research articles, products, social tags, music, etc.

Examples include movie recommenders in IMDB, Netflix & BookMyShow, product recommenders in e-commerce sites like Amazon, eBay & Flipkart, YouTube video recommendations and game recommendations in Xbox.",edureka,,
What is Linear Regression?,Linear regression is a statistical technique where the score of a variable Y is predicted from the score of a second variable X. X is referred to as the predictor variable and Y as the criterion variable.,edureka,,
What is Collaborative filtering?,"The process of filtering used by most of the recommender systems to find patterns or information by collaborating viewpoints, various data sources and multiple agents.

Collaborative Filtering - Data Science Interview Questions - Edureka

An example of collaborative filtering can be to predict the rating of a particular user based on his/her ratings for other movies and others’ ratings for all movies. This concept is widely used in recommending movies in IMDB, Netflix & BookMyShow, product recommenders in e-commerce sites like Amazon, eBay & Flipkart, YouTube video recommendations and game recommendations in Xbox.",edureka,,
How can outlier values be treated?,"Outlier values can be identified by using univariate or any other graphical analysis method. If the number of outlier values is few then they can be assessed individually but for large number of outliers the values can be substituted with either the 99th or the 1st percentile values.

All extreme values are not outlier values. The most common ways to treat outlier values

To change the value and bring in within a range
To just remove the value.",edureka,,
What are various steps involved in an analytics project?,"The following are the various steps involved in an analytics project:

Understand the business problem
Explore the data and become familiar with it.
Prepare the data for modelling by detecting outliers, treating missing values, transforming variables, etc.
After data preparation, start running the model, analyse the result and tweak the approach. This is an iterative step till the best possible outcome is achieved.
Validate the model using a new data set.
Start implementing the model and track the result to analyse the performance of the model over the period of time.",edureka,,
"During analysis, how do you treat missing values?","The extent of the missing values is identified after identifying the variables with missing values. If any patterns are identified the analyst has to concentrate on them as it could lead to interesting and meaningful business insights.

If there are no patterns identified, then the missing values can be substituted with mean or median values (imputation) or they can simply be ignored. Assigning a default value which can be mean, minimum or maximum value. Getting into the data is important.

If it is a categorical variable, the default value is assigned. The missing value is assigned a default value. If you have a distribution of data coming, for normal distribution give the mean value.

If 80% of the values for a variable are missing then you can answer that you would be dropping the variable instead of treating the missing values.",edureka,,
How will you define the number of clusters in a clustering algorithm?,"Though the Clustering Algorithm is not specified, this question is mostly in reference to K-Means clustering where “K” defines the number of clusters. The objective of clustering is to group similar entities in a way that the entities within a group are similar to each other but the groups are different from each other.

For example, the following image shows three different groups. 
Within Sum of squares is generally used to explain the homogeneity within a cluster. If you plot WSS for a range of number of clusters, you will get the plot shown below.
The Graph is generally known as Elbow Curve.
Red circled point in above graph i.e. Number of Cluster =6 is the point after which you don’t see any decrement in WSS.
This point is known as bending point and taken as K in K – Means.
This is the widely used approach but few data scientists also use Hierarchical clustering first to create dendograms and identify the distinct groups from there.",edureka,,
"In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the proba­bility that you see at least one shooting star in the period of an hour?","Probability of not seeing any shooting star in 15 minutes is

=   1 – P( Seeing one shooting star )
=   1 – 0.2          =    0.8

Probability of not seeing any shooting star in the period of one hour

=   (0.8) ^ 4        =    0.4096

Probability of seeing at least one shooting star in the one hour

=   1 – P( Not seeing any star )
=   1 – 0.4096     =    0.5904",edureka,,
How can you generate a random number between 1 – 7 with only a die?,"Any die has six sides from 1-6. There is no way to get seven equal outcomes from a single rolling of a die. If we roll the die twice and consider the event of two rolls, we now have 36 different outcomes.
To get our 7 equal outcomes we have to reduce this 36 to a number divisible by 7. We can thus consider only 35 outcomes and exclude the other one.
A simple scenario can be to exclude the combination (6,6), i.e., to roll the die again if 6 appears twice.
All the remaining combinations from (1,1) till (6,5) can be divided into 7 parts of 5 each. This way all the seven sets of outcomes are equally likely.",edureka,,
"A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?","In the case of two children, there are 4 equally likely possibilities 

BB, BG, GB and GG;

where B = Boy and G = Girl and the first letter denotes the first child.

From the question, we can exclude the first case of BB. Thus from the remaining 3 possibilities of BG, GB & BB, we have to find the probability of the case with two girls.

Thus, P(Having two girls given one girl)   =    1 / 3",edureka,,
"A jar has 1000 coins, of which 999 are fair and 1 is double headed. Pick a coin at random, and toss it 10 times. Given that you see 10 heads, what is the probability that the next toss of that coin is also a head?","There are two ways of choosing the coin. One is to pick a fair coin and the other is to pick the one with two heads.

Probability of selecting fair coin = 999/1000 = 0.999
Probability of selecting unfair coin = 1/1000 = 0.001

Selecting 10 heads in a row = Selecting fair coin * Getting 10 heads  +  Selecting an unfair coin

P (A)  =  0.999 * (1/2)^5  =  0.999 * (1/1024)  =  0.000976
P (B)  =  0.001 * 1  =  0.001
P( A / A + B )  = 0.000976 /  (0.000976 + 0.001)  =  0.4939
P( B / A + B )  = 0.001 / 0.001976  =  0.5061

Probability of selecting another head = P(A/A+B) * 0.5 + P(B/A+B) * 1 = 0.4939 * 0.5 + 0.5061  =  0.7531",edureka,,
Explain to me a technical concept related to the role that you’re interviewing for.,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
Introduce me to something you’re passionate about.,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you explain an A/B test to an engineer with no statistics background? A linear regression?,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you explain a confidence interval to an engi- neer with no statistics background? What does 95% confidence mean?,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you explain to a group of senior executives why data is important?,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
(Given a Dataset) Analyze this dataset and tell me what you can learn from it.,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
What is R2? What are some other metrics that could be better than R2 and why?,"goodness of fit measure. variance explained by the regression / total variance
the more predictors you add the higher R^2 becomes.
hence use adjusted R^2 which adjusts for the degrees of freedom 
or train error metrics",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What is the curse of dimensionality?,"High dimensionality makes clustering hard, because having lots of dimensions means that everything is ""far away"" from each other.
For example, to cover a fraction of the volume of the data we need to capture a very wide range for each variable as the number of variables increases
All samples are close to the edge of the sample. And this is a bad news because prediction is much more difficult near the edges of the training sample.
The sampling density decreases exponentially as p increases and hence the data becomes much more sparse without significantly more data. 
We should conduct PCA to reduce dimensionality",https://github.com/kojino/120-Data-Science-Interview-Questions,,
Is more data always better?,"Statistically,
It depends on the quality of your data, for example, if your data is biased, just getting more data won’t help.
It depends on your model. If your model suffers from high bias, getting more data won’t improve your test results beyond a point. You’d need to add more features, etc.
Practically,
Also there’s a tradeoff between having more data and the additional storage, computational power, memory it requires. Hence, always think about the cost of having more data.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What are advantages of plotting your data before per- forming analysis?,"Data sets have errors.  You won't find them all but you might find some. That 212 year old man. That 9 foot tall woman.
Variables can have skewness, outliers etc.  Then the arithmetic mean might not be useful. Which means the standard deviation isn't useful.

Variables can be multimodal!  If a variable is multimodal then anything based on its mean or median is going to be suspect. ",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How can you make sure that you don’t analyze something that ends up meaningless?,"Proper exploratory data analysis.
In every data analysis task, there's the exploratory phase where you're just graphing things, testing things on small sets of the data, summarizing simple statistics, and getting rough ideas of what hypotheses you might want to pursue further.

Then there's the exploitatory phase, where you look deeply into a set of hypotheses. 

The exploratory phase will generate lots of possible hypotheses, and the exploitatory phase will let you really understand a few of them. Balance the two and you'll prevent yourself from wasting time on many things that end up meaningless, although not all.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What is the role of trial and error in data analysis? What is the the role of making a hypothesis before diving in?,"data analysis is a repetition of setting up a new hypothesis and trying to refute the null hypothesis.
The scientific method is eminently inductive: we elaborate a hypothesis, test it and refute it or not. As a result, we come up with new hypotheses which are in turn tested and so on. This is an iterative process, as science always is.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How can you determine which features are the most im- portant in your model?,"run the features though a Gradient Boosting Machine or Random Forest to generate plots of relative importance and information gain for each feature in the ensembles.
Look at the variables added in forward variable selection ",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How do you deal with some of your predictors being missing?,"Remove rows with missing values - This works well if 1) the values are missing randomly (see Vinay Prabhu's answer for more details on this) 2) if you don't lose too much of the dataset after doing so.
Build another predictive model to predict the missing values - This could be a whole project in itself, so simple techniques are usually used here.
Use a model that can incorporate missing data - Like a random forest, or any tree-based method.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?","Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are highly linearly related. 
Leave the model as is, despite multicollinearity. The presence of multicollinearity doesn't affect the efficiency of extrapolating the fitted model to new data provided that the predictor variables follow the same pattern of multicollinearity in the new data as in the data on which the regression model is based.
principal component regression",https://github.com/kojino/120-Data-Science-Interview-Questions,,
Let’s say you’re given an unfeasible amount of predictors in a predictive modeling task. What are some ways to make the prediction more feasible?,PCA,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Now you have a feasible amount of predictors, but you’re fairly sure that you don’t need all of them. How would you perform feature selection on the dataset?","ridge / lasso / elastic net regression
Univariate Feature Selection where a statistical test is applied to each feature individually. You retain only the best features according to the test outcome scores
""Recursive Feature Elimination"":
First, train a model with all the feature and evaluate its performance on held out data.
Then drop let say the 10% weakest features (e.g. the feature with least absolute coefficients in a linear model) and retrain on the remaining features.
Iterate until you observe a sharp drop in the predictive accuracy of the model.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
Your linear regression didn’t run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?,"p > n.
If some of the explanatory variables are perfectly correlated (positively or negatively) then the coefficients would not be unique. ",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?","The dataset might be heterogeneous. In which case, it is recommended to cluster datasets into different subsets wisely, and then draw different models for different subsets. Or, use models like non parametric models (trees) which can deal with heterogeneity quite nicely.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"What is the main idea behind ensemble learning? If I had many different models that predicted the same response variable, what might I want to do to incorporate all of the models? Would you expect this to perform better than an individual model or worse?","The assumption is that a group of weak learners can be combined to form a strong learner.
Hence the combined model is expected to perform better than an individual model.
Assumptions:
average out biases
reduce variance
Bagging works because some underlying learning algorithms are unstable: slightly different inputs leads to very different outputs. If you can take advantage of this instability by running multiple instances, it can be shown that the reduced instability leads to lower error. If you want to understand why, the original bagging paper( http://www.springerlink.com/cont...) has a section called ""why bagging works""
Boosting works because of the focus on better defining the ""decision edge"". By reweighting examples near the margin (the positive and negative examples) you get a reduced error (see http://citeseerx.ist.psu.edu/vie...)
Use the outputs of your models as inputs to a meta-model. 
For example, if you're doing binary classification, you can use all the probability outputs of your individual models as inputs to a final logistic regression (or any model, really) that can combine the probability estimates.

One very important point is to make sure that the output of your models are out-of-sample predictions. This means that the predicted value for any row in your dataframe should NOT depend on the actual value for that row.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given that you have wi  data in your o ce, how would you determine which rooms and areas are underutilized and overutilized?","If the data is more used in one room, then that one is over utilized! Maybe account for the room capacity and normalize the data.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How could you use GPS data from a car to determine the quality of a driver?,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given accelerometer, altitude, and fuel usage data from a car, how would you determine the optimum acceleration pattern to drive over hills?",,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given position data of NBA players in a season’s games, how would you evaluate a basketball player’s defensive ability?",,https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you quantify the influence of a Twitter user?,like page rank with each user corresponding to the webpages and linking to the page equivalent to following.,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given location data of golf balls in games, how would construct a model that can advise golfers where to aim?",,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"You have 100 mathletes and 100 math problems. Each mathlete gets to choose 10 problems to solve. Given data on who got what problem correct, how would you rank the problems in terms of di culty?","One way you could do this is by storing a ""skill level"" for each user and a ""difficulty level"" for each problem.  We assume that the probability that a user solves a problem only depends on the skill of the user and the difficulty of the problem.*  Then we maximize the likelihood of the data to find the hidden skill and difficulty levels.
The Rasch model for dichotomous data takes the form:
{\displaystyle \Pr\{X_{ni}=1\}={\frac {\exp({\beta _{n}}-{\delta _{i}})}{1+\exp({\beta _{n}}-{\delta _{i}})}},}
where  is the ability of person  and  is the difficulty of item}.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
You have 5000 people that rank 10 sushis in terms of saltiness. How would you aggregate this data to estimate the true saltiness rank in each sushi?,"Some people would take the mean rank of each sushi.  If I wanted something simple, I would use the median, since ranks are (strictly speaking) ordinal and not interval, so adding them is a bit risque (but people do it all the time and you probably won't be far wrong).",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given data on congressional bills and which congressional representatives co-sponsored the bills, how would you determine which other representatives are most similar to yours in voting behavior? How would you evaluate who is the most liberal? Most republican? Most bipartisan?","collaborative filtering. you have your votes and we can calculate the similarity for each representatives and select the most similar representative
for liberal and republican parties, find the mean vector and find the representative closest to the center point",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you come up with an algorithm to detect plagiarism in online content?,"reduce the text to a more compact form (e.g. fingerprinting, bag of words) then compare those with other texts by calculating the similarity",https://github.com/kojino/120-Data-Science-Interview-Questions,,
You have data on all purchases of customers at a grocery store. Describe to me how you would program an algorithm that would cluster the customers into groups. How would you determine the appropriate number of clusters to include?,"KNN
choose a small value of k that still has a low SSE (elbow method)
https://bl.ocks.org/rpgove/0060ff3b656618e9136b",https://github.com/kojino/120-Data-Science-Interview-Questions,,
Let's say you're building the recommended music engine at Spotify to recommend people music based on past listening history. How would you approach this problem?,"collaborative filtering. you have your votes and we can calculate the similarity for each representatives and select the most similar representative
for liberal and republican parties, find the mean vector and find the representative closest to the center point",https://github.com/kojino/120-Data-Science-Interview-Questions,,
(Given a Dataset) Analyze this dataset and give me a model that can predict this response variable.,"Start by fitting a simple model (multivariate regression, logistic regression), do some feature engineering accordingly, and then try some complicated models. Always split the dataset into train, validation, test dataset and use cross validation to check their performance.
Determine if the problem is classification or regression
Favor simple models that run quickly and you can easily explain.
Mention cross validation as a means to evaluate the model.
Plot and visualize the data.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?,"The model that has high training accuracy might have low test accuracy. Without further knowledge, it is hard to know which dataset represents the population data and thus the generalizability of the algorithm is hard to measure. This should be mitigated by repeated splitting of train vs test dataset (as in cross validation).
When there is a change in data distribution, this is called the dataset shift. If the train and test data has a different distribution, then the classifier would likely overfit to the train data.
This issue can be overcome by using a more general learning method.
This can occur when:
P(y|x) are the same but P(x) are different. (covariate shift)
P(y|x) are different. (concept shift)
The causes can be:
Training samples are obtained in a biased way. (sample selection bias)
Train is different from test because of temporal, spatial changes. (non-stationary environments)
Solution to covariate shift
importance weighted cv",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What are some ways I can make my model more robust to outliers?,"We can have regularization such as L1 or L2 to reduce variance (increase bias).
Changes to the algorithm:
Use tree-based methods instead of regression methods as they are more resistant to outliers. For statistical tests, use non parametric tests instead of parametric ones.
Use robust error metrics such as MAE or Huber Loss instead of MSE.
Changes to the data:
Winsorizing the data
Transforming the data (e.g. log)
Remove them only if you’re certain they’re anomalies not worth predicting",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?","MSE is more strict to having outliers. MAE is more robust in that sense, but is harder to fit the model for because it cannot be numerically optimized. So when there are less variability in the model and the model is computationally easy to fit, we should use MAE, and if that’s not the case, we should use MSE.
MSE: easier to compute the gradient, MAE: linear programming needed to compute the gradient
MAE more robust to outliers. If the consequences of large errors are great, use MSE
MSE corresponds to maximizing likelihood of Gaussian random variables",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What error metric would you use to evaluate how good a binary classifier is? What if the classes are imbalanced? What if there are more than 2 groups?,"Accuracy: proportion of instances you predict correctly. Pros: intuitive, easy to explain, Cons: works poorly when the class labels are imbalanced and the signal from the data is weak
AUROC: plot fpr on the x axis and tpr on the y axis for different threshold. Given a random positive instance and a random negative instance, the AUC is the probability that you can identify who's who. Pros: Works well when testing the ability of distinguishing the two classes, Cons: can’t interpret predictions as probabilities (because AUC is determined by rankings), so can’t explain the uncertainty of the model
logloss/deviance: Pros: error metric based on probabilities, Cons: very sensitive to false positives, negatives
When there are more than 2 groups, we can have k binary classifications and add them up for logloss. Some metrics like AUC is only applicable in the binary case.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What’s the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)","Things to look at: N, P, linearly seperable?, features independent?, likely to overfit?, speed, performance, memory usage
Logistic Regression
features roughly linear, problem roughly linearly separable
robust to noise, use l1,l2 regularization for model selection, avoid overfitting
the output come as probabilities
efficient and the computation can be distributed
can be used as a baseline for other algorithms
(-) can hardly handle categorical features
SVM
with a nonlinear kernel, can deal with problems that are not linearly separable
(-) slow to train, for most industry scale applications, not really efficient
Naive Bayes
computationally efficient when P is large by alleviating the curse of dimensionality
works surprisingly well for some cases even if the condition doesn’t hold
with word frequencies as features, the independence assumption can be seen reasonable. So the algorithm can be used in text categorization
(-) conditional independence of every other feature should be met
Tree Ensembles
good for large N and large P, can deal with categorical features very well
non parametric, so no need to worry about outliers
GBT’s work better but the parameters are harder to tune
RF works out of the box, but usually performs worse than GBT
Deep Learning
works well for some classification tasks (e.g. image)
used to squeeze something out of the problem",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What is regularization and where might it be helpful? What is an example of using regularization in a model?,"Regularization is useful for reducing variance in the model, meaning avoiding overfitting . For example, we can use L1 regularization in Lasso regression to penalize large coefficients.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
Why might it be preferable to include fewer predictors over many?,"When we add irrelevant features, it increases model's tendency to overfit because those features introduce more noise. When two variables are correlated, they might be harder to interpret in case of regression, etc.
curse of dimensionality
adding random noise makes the model more complicated but useless
computational cost
Ask someone for more details.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?","Build a time series model with the training data with a seven day cycle and then use that for a new data with only 2 days data.
Ask someone for more details.
Build a regression function to estimate the number of retweets as a function of time t
to determine if one regression function can be built, see if there are clusters in terms of the trends in the number of retweets
if not, we have to add features to the regression function
features + # of retweets on the first and the second day -> predict the seventh day
https://en.wikipedia.org/wiki/Dynamic_time_warping",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How could you collect and analyze data to use social media to predict the weather?,"We can collect social media data using twitter, Facebook, instagram API’s. Then, for example, for twitter, we can construct features from each tweet, e.g. the tweeted date, number of favorites, retweets, and of course, the features created from the tweeted content itself. Then use a multi variate time series model to predict the weather.
Ask someone for more details.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you construct a feed to show relevant content for a site that involves user interactions with items?,"We can do so using building a recommendation engine. The easiest we can do is to show contents that are popular other users, which is still a valid strategy if for example the contents are news articles. To be more accurate, we can build a content based filtering or collaborative filtering. If there’s enough user usage data, we can try collaborative filtering and recommend contents other similar users have consumed. If there isn’t, we can recommend similar items based on vectorization of items (content based filtering).",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you design the people you may know feature on LinkedIn or Facebook?,"Find strong unconnected people in weighted connection graph
Define similarity as how strong the two people are connected
Given a certain feature, we can calculate the similarity based on
friend connections (neighbors)
Check-in’s people being at the same location all the time.
same college, workplace
Have randomly dropped graphs test the performance of the algorithm
ref. News Feed Optimization
Affinity score: how close the content creator and the users are
Weight: weight for the edge type (comment, like, tag, etc.). Emphasis on features the company wants to promote
Time decay: the older the less important",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you predict who someone may want to send a Snapchat or Gmail to?,"for each user, assign a score of how likely someone would send an email to
the rest is feature engineering:
number of past emails, how many responses, the last time they exchanged an email, whether the last email ends with a question mark, features about the other users, etc.
Ask someone for more details.
People who someone sent emails the most in the past, conditioning on time decay.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you suggest to a franchise where to open a new store?,"build a master dataset with local demographic information available for each location.
local income levels, proximity to traffic, weather, population density, proximity to other businesses
a reference dataset on local, regional, and national macroeconomic conditions (e.g. unemployment, inflation, prime interest rate, etc.)
any data on the local franchise owner-operators, to the degree the manager
identify a set of KPIs acceptable to the management that had requested the analysis concerning the most desirable factors surrounding a franchise
quarterly operating profit, ROI, EVA, pay-down rate, etc.
run econometric models to understand the relative significance of each variable
run machine learning algorithms to predict the performance of each location candidate",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"In a search engine, given partial data on what the user has typed, how would you predict the user’s eventual search query?","Based on the past frequencies of words shown up given a sequence of words, we can construct conditional probabilities of the set of next sequences of words that can show up (n-gram). The sequences with highest conditional probabilities can show up as top candidates.
To further improve this algorithm,
we can put more weight on past sequences which showed up more recently and near your location to account for trends
show your recent searches given partial data",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given a database of all previous alumni donations to your university, how would you predict which recent alumni are most likely to donate?","Based on frequency and amount of donations, graduation year, major, etc, construct a supervised regression (or binary classification) algorithm.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
 You’re Uber and you want to design a heatmap to recommend to drivers where to wait for a passenger. How would you approach this?,"Based on the past pickup location of passengers around the same time of the day, day of the week (month, year), construct
Ask someone for more details.
Based on the number of past pickups
account for periodicity (seasonal, monthly, weekly, daily, hourly)
special events (concerts, festivals, etc.) from tweets",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you build a model to predict a March Madness bracket?,"One vector each for team A and B. Take the difference of the two vectors and use that as an input to predict the probability that team A would win by training the model. Train the models using past tournament data and make a prediction for the new tournament by running the trained model for each round of the tournament
Some extensions:
Experiment with different ways of consolidating the 2 team vectors into one (e.g concantenating, averaging, etc)
Consider using a RNN type model that looks at time series data.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"You want to run a regression to predict the probability of a flight delay, but there are flights with delays of up to 12 hours that are really messing up your model. How can you address this?","This is equivalent to making the model more robust to outliers.
See Q3.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Bobo the amoeba has a 25%, 25%, and 50% chance of producing 0, 1, or 2 o spring, respectively. Each of Bobo’s descendants also have the same probabilities. What is the probability that Bobo’s lineage dies out?",p=1/4+1/4p+1/2p^2 => p=1/2,https://github.com/kojino/120-Data-Science-Interview-Questions,,
" In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the proba- bility that you see at least one shooting star in the period of an hour?","1-(0.8)^4. Or, we can use Poisson processes",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How can you generate a random number between 1 - 7 with only a die?,"Launch it 3 times: each throw sets the nth bit of the result.
For each launch, if the value is 1-3, record a 0, else 1. The result is between 0 (000) and 7 (111), evenly spread (3 independent throw). Repeat the throws if 0 was obtained: the process stops on evenly spread values.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How can you get a fair coin toss if someone hands you a coin that is weighted to come up heads more often than tails?,"Flip twice and if HT then H, TH then T.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
You have an 50-50 mixture of two normal distributions with the same standard deviation. How far apart do the means need to be in order for this distribution to be bimodal?,more than two standard deviations,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?",plug in the value to the CDF of the same random variable,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?",3-Jan,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"You have a group of couples that decide to have children until they have their first girl, after which they stop having children. What is the expected gender ratio of the children that are born? What is the expected number of children each couple will have?",gender ratio is 1:1. Expected number of children is 2. let X be the number of children until getting a female (happens with prob 1/2). this follows a geometric distribution with probability 1/2,https://github.com/kojino/120-Data-Science-Interview-Questions,,
How many ways can you split 12 people into 3 teams of 4?,the outcome follows a multinomial distribution with n=12 and k=3. but the classes are indistinguishable,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Your hash function assigns each object to a number between 1:10, each with equal probability. With 10 objects, what is the probability of a hash collision? What is the expected number of hash collisions? What is the expected number of hashes that are unused.","the probability of a hash collision: 1-(10!/10^10)
the expected number of hash collisions: 1-10*(9/10)^10
the expected number of hashes that are unused: 10*(9/10)^10",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"You call 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?","Lyfts arrive first: 2!*3!/5!
Ubers arrive first: same",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"I write a program should print out all the numbers from 1 to 300, but prints out Fizz instead if the number is divisible by 3, Buzz instead if the number is divisible by 5, and FizzBuzz if the number is divisible by 3 and 5. What is the total number of numbers that is either Fizzed, Buzzed, or FizzBuzzed?",100+60-20=140,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"On a dating site, users can select 5 out of 24 adjectives to describe themselves. A match is declared between two users if they match on at least 4 adjectives. If Alice and Bob randomly pick adjectives, what is the probability that they form a match?",24C5*(1+5(24-5))/24C5*24C5 = 4/1771,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"A lazy high school senior types up application and envelopes to n different colleges, but puts the applications randomly into the envelopes. What is the expected number of applications that went to the right college?",1,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Let’s say you have a very tall father. On average, what would you expect the height of his son to be? Taller, equal, or shorter? What if you had a very short father?",Shorter. Regression to the mean,https://github.com/kojino/120-Data-Science-Interview-Questions,,
What’s the expected number of coin flips until you get two heads in a row? What’s the expected number of coin flips until you get two tails in a row?,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Let’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you 2n-1 dollars. How much would you pay me to play this game?",less than $3,https://github.com/kojino/120-Data-Science-Interview-Questions,,
" You have two coins, one of which is fair and comes up heads with a probability 1/2, and the other which is biased and comes up heads with probability 3/4. You randomly pick coin and flip it twice, and get heads both times. What is the probability that you picked the fair coin?",13-Apr,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"You have a 0.1% chance of picking up a coin with both heads, and a 99.9% chance that you pick up a fair coin. You flip your coin and it comes up heads 10 times. What’s the chance that you picked up the fair coin, given the information that you observed?","Events: F = ""picked a fair coin"", T = ""10 heads in a row""
(1) P(F|T) = P(T|F)P(F)/P(T) (Bayes formula)
(2) P(T) = P(T|F)P(F) + P(T|¬F)P(¬F) (total probabilities formula)
Injecting (2) in (1): P(F|T) = P(T|F)P(F)/(P(T|F)P(F) + P(T|¬F)P(¬F)) = 1 / (1 + P(T|¬F)P(¬F)/(P(T|F)P(F)))
Numerically: 1/(1 + 0.001 * 2^10 /0.999).
With 2^10 ≈ 1000 and 0.999 ≈ 1 this simplifies to 1/2",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What is a P-Value ?,"The probability to obtain a similar or more extreme result than observed when the null hypothesis is assumed.
⇒ If the p-value is small, the null hypothesis is unlikely",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"What would be good metrics of success for an advertising-driven consumer product? (Buzzfeed, YouTube, Google Search, etc.) A service-driven consumer product? (Uber, Flickr, Venmo, etc.)","advertising-driven: Pageviews and daily actives, CTR, CPC (cost per click)
click-ads
display-ads
service-driven: number of purchases, conversion rate",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"What would be good metrics of success for a productiv- ity tool? (Evernote, Asana, Google Docs, etc.) A MOOC? (edX, Coursera, Udacity, etc.)","productivity tool: same as premium subscriptions
MOOC: same as premium subscriptions, completion rate",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"What would be good metrics of success for an e-commerce product? (Etsy, Groupon, Birchbox, etc.) A subscrip- tion product? (Net ix, Birchbox, Hulu, etc.) Premium subscriptions? (OKCupid, LinkedIn, Spotify, etc.) ","e-commerce: number of purchases, conversion rate, Hourly, daily, weekly, monthly, quarterly, and annual sales, Cost of goods sold, Inventory levels, Site traffic, Unique visitors versus returning visitors, Customer service phone call count, Average resolution time
subscription
churn, CoCA, ARPU, MRR, LTV
premium subscriptions: ",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"What would be good metrics of success for a consumer product that relies heavily on engagement and interac- tion? (Snapchat, Pinterest, Facebook, etc.) A messaging product? (GroupMe, Hangouts, Snapchat, etc.)","heavily on engagement and interaction: uses AU ratios, email summary by type, and push notification summary by type, resurrection ratio
messaging product: ",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"What would be good metrics of success for a product that o ered in-app purchases? (Zynga, Angry Birds, other gaming apps)","Average Revenue Per Paid User
Average Revenue Per User",https://github.com/kojino/120-Data-Science-Interview-Questions,,
A certain metric is violating your expectations by going down or up more than you expect. How would you try to identify the cause of the change?,"breakdown the KPI’s into what consists them and find where the change is
then further breakdown that basic KPI by channel, user cluster, etc. and relate them with any campaigns, changes in user behaviors in that segment",https://github.com/kojino/120-Data-Science-Interview-Questions,,
Growth for total number of tweets sent has been slow this month. What data would you look at to determine the cause of the problem?,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
You’re a restaurant and are approached by Groupon to run a deal. What data would you ask from them in order to determine whether or not to do the deal?,"for similar restaurants (they should define similarity), average increase in revenue gain per coupon, average increase in customers per coupon",https://github.com/kojino/120-Data-Science-Interview-Questions,,
You are tasked with improving the e ciency of a subway system. Where would you start?,define efficiency,https://github.com/kojino/120-Data-Science-Interview-Questions,,
Say you are working on Facebook News Feed. What would be some metrics that you think are important? How would you make the news each person gets more relevant?,"rate for each action, duration users stay, CTR for sponsor feed posts
ref. News Feed Optimization
Affinity score: how close the content creator and the users are
Weight: weight for the edge type (comment, like, tag, etc.). Emphasis on features the company wants to promote
Time decay: the older the less important",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you measure the impact that sponsored stories on Facebook News Feed have on user engagement? How would you determine the optimum balance between sponsored stories and organic content on a user’s News Feed?,AB test on different balance ratio and see ,https://github.com/kojino/120-Data-Science-Interview-Questions,,
You are on the data science team at Uber and you are asked to start thinking about surge pricing. What would be the objectives of such a product and how would you start looking into this?,"there is a gradual step-function type scaling mechanism until that imbalance of requests-to-drivers is alleviated and then vice versa as too many drivers come online enticed by the surge pricing structure. 
I would bet the algorithm is custom tailored and calibrated to each location as price elasticities almost certainly vary across different cities depending on a huge multitude of variables: income, distance/sprawl, traffic patterns, car ownership, etc. With the massive troves of user data that Uber probably has collected, they most likely have tweaked the algos for each city to adjust for these varying sensitivities to surge pricing. Throw in some machine learning and incredibly rich data and you've got yourself an incredible, constantly-evolving algorithm.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
Say that you are Net ix. How would you determine what original series you should invest in and create?,Netflix uses data to estimate the potential market size for an original series before giving it the go-ahead.,https://github.com/kojino/120-Data-Science-Interview-Questions,,
What kind of services would  nd churn (metric that tracks how many customers leave the service) helpful? How would you calculate churn?,subscription based services,https://github.com/kojino/120-Data-Science-Interview-Questions,,
Let’s say that you’re are scheduling content for a content provider on television. How would you determine the best times to schedule content?Â,,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Write a function to calculate all possible assignment vectors of 2n users, where n users are assigned to group 0 (control), and n users are assigned to group 1 (treatment).",Recursive programming (sol in code),https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given a list of tweets, determine the top 10 most used hashtags.",Store all the hashtags in a dictionary and get the top 10 values,https://github.com/kojino/120-Data-Science-Interview-Questions,,
Program an algorithm to find the best approximate solution to the knapsack problem1 in a given time.,Greedy solution (add the best v/w as much as possible and move on to the next),https://github.com/kojino/120-Data-Science-Interview-Questions,,
Program an algorithm to find the best approximate solution to the travelling salesman problem2 in a given time.,Greedy solution (add the best v/w as much as possible and move on to the next),https://github.com/kojino/120-Data-Science-Interview-Questions,,
"You have a stream of data coming in of size n, but you don’t know what n is ahead of time. Write an algorithm that will take a random sample of k elements. Can you write one that takes O(k) space?",https://en.wikipedia.org/wiki/Reservoir_sampling,https://github.com/kojino/120-Data-Science-Interview-Questions,,
Write an algorithm that can calculate the square root of a number.,"https://www.quora.com/What-is-the-method-to-calculate-a-square-root-by-hand?redirected_qid=664405
https://en.wikipedia.org/wiki/Newton's_method#Square_root_of_a_number",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given a list of numbers, can you return the outliers?",sort then select the highest and the lowest 2.5%,https://github.com/kojino/120-Data-Science-Interview-Questions,,
When can parallelism make your algorithms run faster?,"When could it make your algorithms run slower?

Ask someone for more details.
compute in parallel when communication cost < computation cost
ensemble trees
minibatch
cross validation
forward propagation
minibatch
not suitable for online learning",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What are the different types of joins? What are the differences between them?,"(INNER) JOIN: Returns records that have matching values in both tables LEFT (OUTER) JOIN: Return all records from the left table, and the matched records from the right table RIGHT (OUTER) JOIN: Return all records from the right table, and the matched records from the left table FULL (OUTER) JOIN: Return all records when there is a match in either left or right table",https://github.com/kojino/120-Data-Science-Interview-Questions,,
Why might a join on a subquery be slow? How might you speed it up?,Change the subquery to a join.,https://github.com/kojino/120-Data-Science-Interview-Questions,,
Describe the difference between primary keys and foreign keys in a SQL database.,Primary keys are columns whose value combinations must be unique in a specific table so that each row can be referenced uniquely. Foreign keys are columns that references columns (often primary keys) in other tables.,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given a COURSES table with columns course_id and course_name, a FACULTY table with columns faculty_id and faculty_name, and a COURSE_FACULTY table with columns faculty_id and course_id, how would you return a list of faculty who teach a course given the name of a course?",select faculty_name from faculty_id c join (select faculty_id from (select course_id from COURSES where course_name=xxx) as a join COURSE_FACULTY b on a.course_id = b.course_id) d on c.faculty_id = d.faculty_id,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Given a IMPRESSIONS table with ad_id, click (an indicator that the ad was clicked), and date, write a SQL query that will tell me the click-through-rate of each ad by month.","select ad_id, average(click) from IMPRESSIONS group by ad_id, month(date)",https://github.com/kojino/120-Data-Science-Interview-Questions,,
Write a query that returns the name of each department and a count of the number of employees in each:,"EMPLOYEES containing: Emp_ID (Primary key) and Emp_Name
EMPLOYEE_DEPT containing: Emp_ID (Foreign key) and Dept_ID (Foreign key)
DEPTS containing: Dept_ID (Primary key) and Dept_Name

select Dept_Name, count(1) from DEPTS a right join EMPLOYEE_DEPT b on a.Dept_id = b.Dept_id group by Dept_Name",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"In an A/B test, how can you check if assignment to the various buckets was truly random?","Plot the distributions of multiple features for both A and B and make sure that they have the same shape. More rigorously, we can conduct a permutation test to see if the distributions are the same.
MANOVA to compare different means",https://github.com/kojino/120-Data-Science-Interview-Questions,,
" What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?",Verify the sampling algorithm is random.,https://github.com/kojino/120-Data-Science-Interview-Questions,,
What would be the hazards of letting users sneak a peek at the other bucket in an A/B test?,"The user might not act the same suppose had they not seen the other bucket. You are essentially adding additional variables of whether the user peeked the other bucket, which are not random across groups.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What would be some issues if blogs decide to cover one of your experimental groups?,Same as the previous question. The above problem can happen in larger scale.,https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you conduct an A/B test on an opt-in feature? ,Ask someone for more details.,https://github.com/kojino/120-Data-Science-Interview-Questions,,
"How would you run an A/B test for many variants, say 20 or more?","one control, 20 treatment, if the sample size for each group is big enough.
Ways to attempt to correct for this include changing your confidence level (e.g. Bonferroni Correction) or doing family-wide tests before you dive in to the individual metrics (e.g. Fisher's Protected LSD).",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you run an A/B test if the observations are extremely right-skewed?,"lower the variability by modifying the KPI
cap values
percentile metrics
log transform
https://www.quora.com/How-would-you-run-an-A-B-test-if-the-observations-are-extremely-right-skewed",https://github.com/kojino/120-Data-Science-Interview-Questions,,
I have two different experiments that both change the sign-up button to my website. I want to test them at the same time. What kinds of things should I keep in mind?,exclusive -> ok,https://github.com/kojino/120-Data-Science-Interview-Questions,,
What is a p-value? What is the di erence between type-1 and type-2 error?,"type-1 error: rejecting Ho when Ho is true

type-2 error: not rejecting Ho when Ha is true",https://github.com/kojino/120-Data-Science-Interview-Questions,,
You are AirBnB and you want to test the hypothesis that a greater number of photographs increases the chances that a buyer selects the listing. How would you test this hypothesis?,"For randomly selected listings with more than 1 pictures, hide 1 random picture for group A, and show all for group B. Compare the booking rate for the two groups.
Ask someone for more details",https://github.com/kojino/120-Data-Science-Interview-Questions,,
How would you design an experiment to determine the impact of latency on user engagement?,"The best way I know to quantify the impact of performance is to isolate just that factor using a slowdown experiment, i.e., add a delay in an A/B test.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What is maximum likelihood estimation? Could there be any case where it doesn’t exist?,"A method for parameter optimization (fitting a model). We choose parameters so as to maximize the likelihood function (how likely the outcome would happen given the current data and our model).
maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters. MLE can be seen as a special case of the maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters, or as a variant of the MAP that ignores the prior and which therefore is unregularized.
for gaussian mixtures, non parametric models, it doesn’t exist",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"What’s the di erence between a MAP, MOM, MLE estima- tor? In which cases would you want to use each?","MAP estimates the posterior distribution given the prior distribution and data which maximizes the likelihood function. MLE is a special case of MAP where the prior is uninformative uniform distribution.
MOM sets moment values and solves for the parameters. MOM is not used much anymore because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What is a confidence interval and how do you interpret it?,"For example, 95% confidence interval is an interval that when constructed for a set of samples each sampled in the same way, the constructed intervals include the true mean 95% of the time.
if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
What is unbiasedness as a property of an estimator? Is this always a desirable property when performing inference? What about in data analysis or predictive modeling?,"Unbiasedness means that the expectation of the estimator is equal to the population value we are estimating. This is desirable in inference because the goal is to explain the dataset as accurately as possible. However, this is not always desirable for data analysis or predictive modeling as there is the bias variance tradeoff. We sometimes want to prioritize the generalizability and avoid overfitting by reducing variance and thus increasing bias.",https://github.com/kojino/120-Data-Science-Interview-Questions,,
"Using Python, write a program/function that prints the least integer that is not present in a given list and cannot be represented by the summation of the sub-elements of the list.",,https://medium.com/acing-ai/top-data-science-interview-questions-answers-part-2-20f8c458056d,,
Is more data always better?,"The answer to these kind of questions is to think logically at different levels.

At a fundamental level having more data means additional storage, more computational power and memory requirement. Hence, there is a cost related to more data. Doing a cost to benefit analysis of this scenario can help make an informed decision.

At a more specific level, considering the nature of the data, quality is an important metric. If your data is biased, just getting more data is of no use.

From a model perspective, we need to consider what additional data does to the existing model. If a model suffers from a high bias, more data will not be able to improve test results beyond a limit unless more features are added.",https://medium.com/acing-ai/top-data-science-interview-questions-answers-part-2-20f8c458056d,,
"What is the difference between an inner join, left join/right join, and full join?","For this question lets us take an example of having two database tables:

Contains the name and unique Ids of all the people who love Pizza.
Contains the name and unique Ids of all the people who are software developers.
Inner Join- This consists of people who are software developers and love Pizza at the same time.

Left Join- This consists of all the people who love pizza who may/may not be software developers.

Right Join- This consists of all software developers who may/may not love Pizza.

Full Join- This consists of all the people from both tables.

To read more: Difference between SQL joins",https://medium.com/acing-ai/top-data-science-interview-questions-answers-part-2-20f8c458056d,,
Write a query that returns the name of each department and a count of the number of employees in each.,"EMPLOYEES containing: Emp_ID (Primary key) and Emp_Name
EMPLOYEE_DEPT containing: Emp_ID (Foreign key) and Dept_ID (Foreign key)
DEPTS containing: Dept_ID (Primary key) and Dept_Name

Select Dept_Name, count(1)

from DEPTS a right join EMPLOYEE_DEPT b on a.Dept_id = b.Dept_id

Group By Dept_Name",https://medium.com/acing-ai/top-data-science-interview-questions-answers-part-2-20f8c458056d,,
What is regularization? Explain L1 and L2 regularization.,"Regularization basically adds penalty to a model as complexity increases. Regularization parameter penalizes all the parameters except intercept so that model generalizes the data. This prevents overfitting.

Both L1 and L2 regularization use penalty to avoid overfitting. The major difference between the two is the way penalty is defined. L1 or Lasso and L2 or Ridge regularization will both reduce/remove features from the model when applied.

Lasso Regression adds “absolute value (magnitude)” of coefficient as penalty term to the loss function. Here we use absolute value as highlighted.


Ridge regression adds “squared value (magnitude)” of coefficient as penalty term to the loss function.


Due to their respective coefficients, L1 regularization is more tolerant of outliers. L1 is better with noisy data and used extensively for the same.",https://medium.com/acing-ai/top-data-science-interview-questions-answers-part-2-20f8c458056d,,
Explain the term botnet?,A botnet is a a type of bot running on an IRC network that has been created with a Trojan.,tekslate,,
What is Data Visualization?,Data visualization is a common term that describes any effort to help people understand the significance of data by placing it in a visual context.,tekslate,,
How you can define Data cleaning as a critical part of process?,"Cleaning up data to the point where you can work with it is a huge amount of work. If we’re trying to reconcile a lot of sources of data that we don’t control like in this flight, it can take 80% of our time.",tekslate,,
Point out 7 Ways how Data Scientists use Statistics?,"Design and interpret experiments to inform product decisions.

Build models that predict signal, not noise.

Turn big data a into the big picture

Understand user retention, engagement, conversion, and leads.

Give your users what they want.

Estimate intelligently.

Tell the story with the data.",tekslate,,
Differentiate between Data modeling and Database design?,"Data Modeling – Data modeling (or modeling) in software engineering is the process of creating a data model for an information system by applying formal data modeling techniques.
Database Design- Database design is the system of producing a detailed data model of a database. The term database design can be used to describe many different parts of the design of an overall database system.",tekslate,,
Describe in brief the data Science Process flowchart?,"Data is collected from sensors in the environment.

Data is “cleaned” or it can process to produce a data set (typically a data table) usable for processing.

Exploratory data analysis and statistical modeling may be performed.

A data product is a program such as retailers use to inform new purchases based on purchase history. It may also create data and feed it back into the environment.",tekslate,,
What are Recommender Systems?,"A subclass of information filtering systems that are meant to predict the preferences or ratings that a user would give to a product. Recommender systems are widely used in movies, news, research articles, products, social tags, music, etc.",tekslate,,
Why data cleaning plays a vital role in analysis?,"Cleaning data from multiple sources to transform it into a format that data analysts or data scientists can work with is a cumbersome process because – as the number of data sources increases, the time take to clean the data increases exponentially due to the number of sources and the volume of data generated in these sources. It might take up to 80% of the time for just cleaning data making it a critical part of analysis task.",tekslate,,
What is Linear Regression?,Linear regression is a statistical technique where the score of a variable Y is predicted from the score of a second variable X. X is referred to as the predictor variable and Y as the criterion variable.,tekslate,,
What do you understand by term hash table collisions?,"Hash table (hash map) is a kind of data structure used to implement an associative array, a structure that can map keys to values. Ideally, the hash function will assign each key to a unique bucket, but sometimes it is possible that two keys will generate an identical hash causing both keys to point to the same bucket. It is known as hash collisions.",tekslate,,
Compare and contrast R and SAS?                                                                                                                                                                                                                                                                                                                                                                                                                               ,"SAS is commercial software whereas R is free source and can be downloaded by anyone.
SAS is easy to learn and provide easy option for people who already know SQL whereas R is a low level programming language and hence simple procedures takes longer codes.",tekslate,,
What do you understand by letter ‘R’?,R is a low level language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at BELL.,tekslate,,
What is Interpolation and Extrapolation?,Estimating a value from 2 unknown values from a list of values is Interpolation. Extrapolation is approximating a value by extending a known set of values or facts.,tekslate,,
What is Collaborative filtering?,"The process of filtering used by most of the recommender systems to find patterns or information by collaborating viewpoints, various data sources and multiple agents.",tekslate,,
What is the difference between Cluster and Systematic Sampling?,"Cluster sampling is a technique used when it becomes difficult to study the target population spread across a wide area and simple random sampling cannot be applied. Cluster Sample is a probability sample where each sampling unit is a collection, or cluster of elements. Systematic sampling is a statistical technique where elements are selected from an ordered sampling frame. In systematic sampling, the list is progressed in a circular manner so once you reach the end of the list,it is progressed from the top again. The best example for systematic sampling is equal probability method.",tekslate,,
Are expected value and mean value different?,They are not different but the terms are used in different contexts. Mean is generally referred when talking about a probability distribution or sample population whereas expected value is generally referred in a random variable context.,tekslate,,
What does P-value signify about the statistical data?,"P-value is used to determine the significance of results after a hypothesis test in statistics. P-value helps the readers to draw conclusions and is always between 0 and 1.

 -P- Value > 0.05 denotes weak evidence against the null hypothesis which means the null hypothesis cannot be rejected.

-P-value <= 0.05 denotes strong evidence against the null hypothesis which means the null hypothesis can be rejected.

-P-value=0.05is the marginal value indicating it is possible to go either way.",tekslate,,
Do gradient descent methods always converge to same point?,"No, they do not because in some cases it reaches a local minima or a local optima point. You don’t reach the global optima point. It depends on the data and starting conditions.",tekslate,,
What is the goal of A/B Testing?,It is a statistical hypothesis testing for randomized experiment with two variables A and B. The goal of A/B Testing is to identify any changes to the web page to maximize or increase the outcome of an interest. An example for this could be identifying the click through rate for a banner ad.,tekslate,,
What is an Eigenvalue and Eigenvector?,"Eigenvectors are used for understanding linear transformations. In data analysis, we usually calculate the eigenvectors for a correlation or covariance matrix. Eigenvectors are the directions along which a particular linear transformation acts by flipping, compressing or stretching. Eigenvalue can be referred to as the strength of the transformation in the direction of eigenvector or the factor by which the compression occurs.",tekslate,,
How can outlier values be treated?,"Outlier values can be identified by using univariate or any other graphical analysis method. If the number of outlier values is few then they can be assessed individually but for large number of outliers the values can be substituted with either the 99th or the 1st percentile values. All extreme values are not outlier values.The most common ways to treat outlier values –

-To change the value and bring in within a range

-To just remove the value.",tekslate,,
How can you assess a good logistic model?,"There are various methods to assess the results of a logistic regression analysis-

•           Using Classification Matrix to look at the true negatives and false positives.

•           Concordance that helps identify the ability of the logistic model to differentiate between the event happening and not happening.

•           Lift helps assess the logistic model by comparing it with random selection.",tekslate,,
What are various steps involved in an analytics project?,"  Understand the business problem

•           Explore the data and become familiar with it.

•           Prepare the data for modelling by detecting outliers, treating missing values, transforming variables, etc.

•           After data preparation, start running the model, analyse the result and tweak the approach. This is an iterative step till the best possible outcome is achieved.

•           Validate the model using a new data set.

•           Start implementing the model and track the result to analyse the performance of the model over the period of time.",tekslate,,
"During analysis, how do you treat missing values?","The extent of the missing values is identified after identifying the variables with missing values. If any patterns are identified the analyst has to concentrate on them as it could lead to interesting and meaningful business insights. If there are no patterns identified, then the missing values can be substituted with mean or median values (imputation) or they can simply be ignored.There are various factors to be considered when answering this question-

-Understand the problem statement, understand the data and then give the answer.Assigning a default value which can be mean, minimum or maximum value. Getting into the data is important.

-If it is a categorical variable, the default value is assigned. The missing value is assigned a default value.

-If you have a distribution of data coming, for normal distribution give the mean value.

-Should we even treat missing values is another important point to consider? If 80% of the values for a variable are missing then you can answer that you would be dropping the variable instead of treating the missing values.",tekslate,,
What is Machine Learning?,"The simplest way to answer this question is – we give the data and equation to the machine. Ask the machine to look at the data and identify the coefficient values in an equation.

For example for the linear regression y=mx+c, we give the data for the variable x, y and the machine learns about the values of m and c from the data.",tekslate,,
Can you explain the difference between a Test Set and a Validation Set?,"Validation set can be considered as a part of the training set as it is used for parameter selection and to avoid Overfitting of the model being built. On the other hand, test set is used for testing or evaluating the performance of a trained machine leaning model.

In simple terms ,the differences can be summarized as-

Training Set is to fit the parameters i.e. weights.

Test Set is to assess the performance of the model i.e. evaluating the predictive power and generalization.

Validation set is to tune the parameters.",tekslate,,
Python or R – Which one would you prefer for text analytics?,The best possible answer for this would be Python because it has Pandas library that provides easy to use data structures and high performance data analysis tools.,tekslate,,
What is logistic regression? Or State an example when you have used logistic regression recently.,"Logistic Regression often referred as logit model is a technique to predict the binary outcome from a linear combination of predictor variables. For example, if you want to predict whether a particular political leader will win the election or not. In this case, the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). The predictor variables here would be the amount of money spent for election campaigning of a particular candidate, the amount of time spent in campaigning, etc.",tekslate,,
"Differentiate between univariate, bivariate and multivariate analysis.","These are descriptive statistical analysis techniques which can be differentiated based on the number of variables involved at a given point of time. For example, the pie charts of sales based on territory involve only one variable and can be referred to as univariate analysis.

If the analysis attempts to understand the difference between 2 variables at time as in a scatterplot, then it is referred to as bivariate analysis. For example, analysing the volume of sale and a spending can be considered as an example of bivariate analysis.

Analysis that deals with the study of more than two variables to understand the effect of variables on the responses is referred to as multivariate analysis.",tekslate,,
Define some key performance indicators for the product,"After playing around with the product, think about this: what are some of the key metrics that the product might want to optimize? Part of a data scientist’s role in certain companies involve working closely with the product teams to help define, measure, and report on these metrics. This is an exercise you can go through by yourself at home, and can really help during your interview process.",tekslate,,
"You are compiling a report for user content uploaded every month and notice a spike in uploads in October. In particular, a spike in picture uploads. What might you think is the cause of this, and how would you test it?",Hypothesis: the photos are Halloween pictures. Test: look at upload trends in countries that do not observe Halloween as a sort of counter-factual analysis.  We cannot say what has caused the spike since causal relationship cannot be established with observed data. But we can compare the averages of all the months by performing a hypothesis testing and rejecting the null hypothesis if the F1 score is significant.,glassdoor,,
"How do you take millions of users with 100's of transactions each, amongst 10k's of products and group the users together in a meaningful segments?",,glassdoor,,
How would you build and test a metric to compare two user's ranked lists of movie/tv show preferences?,,glassdoor,,
"How would you test if survey responses were filled at random by certain individuals, as opposed to truthful selections?",,glassdoor,,
The three data structure questions are: 1. the difference between linked list and array; 2. the difference between stack and queue; 3. describe hash table.,,glassdoor,,
can you host multiple https sites with different domain names but same IP on the same server? how,,glassdoor,,
"You're about to get on a plane to Seattle. You want to know if you should bring an umbrella. You call 3 random friends of yours who live there and ask each independently if it's raining. Each of your friends has a 2/3 chance of telling you the truth and a 1/3 chance of messing with you by lying. All 3 friends tell you that ""Yes"" it is raining. What is the probability that it's actually raining in Seattle?",,glassdoor,,
Write a function that takes in two sorted lists and outputs a sorted list that is their union.,,glassdoor,,
"generating a sorted vector from two sorted vectors.

3 Answers
",,glassdoor,,
How do you test whether a new credit risk scoring model works? What data would you look at?,,glassdoor,,
Bayes' Formular: Marbles: There are 30 red marbles and 10 black marbles in Urn #1. You have 20 red and 20 Black marbles in Urn 2. Randomly you pull a marble from the random urn and find that it is red. What is the probability that it was pulled from Urn #1,,glassdoor,,
"Two random cards numbered from 1,2...100 are pulled from the deck. What is the probability that one number doubles the other from the deck.",,glassdoor,,
"Alice and Bob take turns in rolling a fair dice. Whoever gets ""6"" first wins the game. Alice starts the game. What are the chances that Alice wins.",,glassdoor,,
How would you sort a billion integers? Merge sort algorithm Palindrome test using recursion. Statistics: A/B testing process,,glassdoor,,
What are two prime numbers that SUM to 999,,glassdoor,,
How do you know if one algorithm is better than other?,,glassdoor,,
"I was given a Data set which had id, position_id and resume_title. I had some junk data , which needs to cleaned up and they expect you to do it onsite while two people watch you doing it. Felt like a ridiculous interview ( people watching you ) what you type , if you google for a command or not. I felt that they are not smart people thats why their interview process is like that. I wouldn't work there ever.",,glassdoor,,
"How do you find out trending queries/topics? How do you test a website feature i.e. given a set of webpages and few changes, how will you find out that the change works positively?",,glassdoor,,
As a part of a bigger question: &#034;How do you draw a uniform random sample from a circle in polar coordinates?&#034;,,glassdoor,,
"given n samples from a uniform distribution [0, d], how to estimate d?",,glassdoor,,
What are the assumptions of linear regression?,,glassdoor,,
What is stationary signal?,"A signal that its frequency content changes over time (over one direction). In essence, a signal that its mean/variance changes with respect to another variable. It is one of the ""diagnosis techniques"" of the linear regression. This is also called the constant variance of the residual.",glassdoor,,
"Given a set of specifications, make a program that would find if a given list of words was in a provided grid of letters",,glassdoor,,
Why is data important?,,glassdoor,,
How would you like to use data to change the world,,glassdoor,,
What is an n-gram?,,glassdoor,,
"Imagine you have N pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?",,glassdoor,,
"Given an array of integers, find the maximum cumulative sum of a sub-set of the array",,glassdoor,,
"How many people must be gathered together in a room, before you can be certain that there is a greater than 50/50 chance that at least two of them have the same birthday?",,glassdoor,,
"Given an existing set of purchases, how do you predict the next item to purchase of a new basket?",,glassdoor,,
"There are 25 horses. You can race any 5 of them at once, and all you get is the order they finished. How many races would you need to find the 3 fastest horses?",,glassdoor,,
